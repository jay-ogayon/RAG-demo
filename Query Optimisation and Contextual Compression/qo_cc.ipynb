{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "def load_documents(directory):\n",
    "    loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "documents = load_documents(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chunk documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: List[Dict[str, Any]], chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunk_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(chunk_docs)} chunks\")\n",
    "    return chunk_docs\n",
    "\n",
    "chunk_docs = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI, OpenAI\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "def setup_vector_store(chunk_docs: List[Dict[str, Any]], index_name: str) -> PineconeVectorStore:\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    \n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "        chunk_docs,\n",
    "        embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "vector_store = setup_vector_store(chunk_docs, \"qo-cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setup and test \"simple retriever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_retriever(query):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "query = \"What is COMPACT in RAG?\"\n",
    "\n",
    "retriever = setup_simple_retriever(query)\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Document {idx}: \\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Query Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "def optimize_query_with_context(query: str, vector_store: PineconeVectorStore):\n",
    "    # First, get relevant documents from the vector store\n",
    "    base_retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    relevant_docs = base_retriever.invoke(query)\n",
    "    \n",
    "    # Extract content from relevant documents\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Create a prompt that includes the context\n",
    "    optimization_template = \"\"\"Given the following context and user query, optimize the query for retrieval. \n",
    "    Focus on key concepts that are present in both the query and the context. \n",
    "    Expand on these concepts using terminology from the context to improve retrieval effectiveness.\n",
    "    Do not provide an answer to the query, only optimize it for better retrieval.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User query: {query}\n",
    "    \n",
    "    Optimized query:\"\"\"\n",
    "    \n",
    "    optimization_prompt = PromptTemplate(\n",
    "        template=optimization_template, \n",
    "        input_variables=['context', 'query']\n",
    "    )\n",
    "    \n",
    "    llm = OpenAI()  \n",
    "    \n",
    "    # Use the pipe operator to create the chain\n",
    "    optimization_chain = optimization_prompt | llm\n",
    "    \n",
    "    # Generate the optimized query\n",
    "    optimized_query = optimization_chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return optimized_query.strip()\n",
    "\n",
    "optimized_query = optimize_query_with_context(query, vector_store)\n",
    "print(f\"Original query: {query}\")\n",
    "print(f\"\\nOptimized query: {optimized_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Setup Compression Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "def setup_compression_retriever(vector_store: PineconeVectorStore) -> ContextualCompressionRetriever:\n",
    "    base_retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "compression_retriever = setup_compression_retriever(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Test Compression Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(query)\n",
    "print(\"Compressed Retrieval Results:\")\n",
    "for i, doc in enumerate(compressed_docs, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(doc.page_content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Setup QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def setup_qa_chain(compression_retriever: ContextualCompressionRetriever) -> RetrievalQA:\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "                If you don't know the answer, just say that you don't know. Do not try to make up an answer.\n",
    "                \n",
    "                {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                Answer:\"\"\",\n",
    "                input_variables=['context', 'question']\n",
    "            ),\n",
    "        },\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "qa_chain = setup_qa_chain(compression_retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Run QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({'query': optimized_query})\n",
    "\n",
    "print(f\"Original query: {query}\")\n",
    "print(f\"Oprimised query: {optimized_query}\")\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(result['result'])\n",
    "\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\nSource Document {i}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
