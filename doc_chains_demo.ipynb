{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "doc = PyPDFDirectoryLoader(\"data/\").load()\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chunk data\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "chunk_data = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap =100).split_documents(doc)\n",
    "len(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001E459E42960>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001E459E42150>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialise embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create VectorStore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunk_data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arman\\OneDrive\\Documents\\GenAI\\RAG-demo\\FAISS\\myvenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='and deployment of LLMs. We are embracing the open source ethos of releasing early and often to \\nenable the community to get access to these models while they are still in development. The text -\\nbased models we are releasing today are the first in the Llama 3 collection of models. Our goal in \\nthe near future is to make Llama 3 multilingual and multimodal, have longer context, and continue \\nto improve overall performance across core LLM capabilities such as reasoning and coding.  \\n \\n \\nState -of-the-art performance  \\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new \\nstate -of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post -\\ntraining, our pretrained and instruction -fine-tuned models are the best models existing today at the \\n8B and 70B parameter scale. Improvements in our post -training procedures substantially reduced', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 1}),\n",
       " Document(page_content='of the community. We want to kickstart the next wave of innovation in AI across the stack —from \\napplications to developer tools to evals to inference optimizations and more. We can’t wait to see \\nwhat you build and look forward to your feedback.  \\n \\nOur goals for Llama 3  \\n \\nWith Llama 3, we set out to build the best open models that are on par with the best proprietary \\nmodels available today. We wanted to address developer feedback to increase the overall \\nhelpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 0}),\n",
       " Document(page_content='8B and 70B parameter scale. Improvements in our post -training procedures substantially reduced \\nfalse refusal rates, improved alignment, and increased diversity in model responses. We also saw \\ngreatly improved capabilities like reasoning, code generation, and instruction following making Llama \\n3 more steerable.  \\n \\n \\n*Please see  evaluation details  for setting and parameters with which these evaluations are calculated.  \\n \\n \\nIn the development of Llama 3, we looked at model performance on standard benchmarks and also \\nsought to optimize for performance for real -world scenarios. To this end, we developed a new high -\\nquality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: \\nasking for advice, brainstorming, classification, closed question answering, coding, creative writing,', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 1}),\n",
       " Document(page_content='Introducing Meta Llama 3: The most \\ncapable openly available LLM to dat e \\n \\nTakeaways:  \\n• Today, we’re introducing Meta Llama 3, the next generation of our state -of-the-art open \\nsource large language model.  \\n• Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, \\nKaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support \\nfrom hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.  \\n• We’re dedicated to developing Llama 3 in a responsible way, and we’re offering various \\nresources to help others use it responsibly as well. This includes introducing new trust \\nand safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.  \\n• In the coming months, we expect to introduce new capabilities, longer context windows, \\nadditional model sizes, and enhanced performance, and we’ll share the Llama 3 research \\npaper.', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create retriever\n",
    "query = \"What are the key improvements of Llama3 from its predecessors?\"\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='and deployment of LLMs. We are embracing the open source ethos of releasing early and often to \\nenable the community to get access to these models while they are still in development. The text -\\nbased models we are releasing today are the first in the Llama 3 collection of models. Our goal in \\nthe near future is to make Llama 3 multilingual and multimodal, have longer context, and continue \\nto improve overall performance across core LLM capabilities such as reasoning and coding.  \\n \\n \\nState -of-the-art performance  \\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new \\nstate -of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post -\\ntraining, our pretrained and instruction -fine-tuned models are the best models existing today at the \\n8B and 70B parameter scale. Improvements in our post -training procedures substantially reduced', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 1}),\n",
       "  0.8031628799518755),\n",
       " (Document(page_content='of the community. We want to kickstart the next wave of innovation in AI across the stack —from \\napplications to developer tools to evals to inference optimizations and more. We can’t wait to see \\nwhat you build and look forward to your feedback.  \\n \\nOur goals for Llama 3  \\n \\nWith Llama 3, we set out to build the best open models that are on par with the best proprietary \\nmodels available today. We wanted to address developer feedback to increase the overall \\nhelpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 0}),\n",
       "  0.7977463355686494),\n",
       " (Document(page_content='8B and 70B parameter scale. Improvements in our post -training procedures substantially reduced \\nfalse refusal rates, improved alignment, and increased diversity in model responses. We also saw \\ngreatly improved capabilities like reasoning, code generation, and instruction following making Llama \\n3 more steerable.  \\n \\n \\n*Please see  evaluation details  for setting and parameters with which these evaluations are calculated.  \\n \\n \\nIn the development of Llama 3, we looked at model performance on standard benchmarks and also \\nsought to optimize for performance for real -world scenarios. To this end, we developed a new high -\\nquality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: \\nasking for advice, brainstorming, classification, closed question answering, coding, creative writing,', metadata={'source': 'data\\\\Meta Llama 3.pdf', 'page': 1}),\n",
       "  0.7951296274054263)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_score = vector_store.similarity_search_with_relevance_scores(query, k=3)\n",
    "search_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arman\\OneDrive\\Documents\\GenAI\\RAG-demo\\FAISS\\myvenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The key improvements of Llama3 from its predecessors include a major leap in performance, establishment of a new state-of-the-art for LLM models at the 8B and 70B parameter scale, improvements in pretraining and post-training procedures, reduced false refusal rates, improved alignment and increased diversity in model responses, and enhanced capabilities such as reasoning, code generation, and instruction following.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crete DocumentsChains\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=api_key, temperature=0)\n",
    "\n",
    "stuff_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                          chain_type = \"stuff\",\n",
    "                                          retriever = retriever)\n",
    "\n",
    "stuff_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe key improvements of Llama3 from its predecessors include: \\n1. Embracing the open source ethos and releasing models early and often for community access. This allows for faster innovation and improvement through collaboration with the community. \\n2. Making Llama3 multilingual and multimodal, allowing for a wider range of applications and use cases. \\n3. Increasing the model size to 8B and 70B parameters, establishing a new state-of-the-art for LLM models at those scales. This allows for more complex and accurate language understanding and generation. \\n4. Improvements in pretraining and post-training procedures, resulting in the best models at the 8B and 70B parameter scale. This leads to better performance and results in various tasks. \\n5. Substantial reduction in post-training procedures, leading to improved performance in core LLM capabilities such as reasoning and coding. This makes Llama3 more efficient and effective in handling complex language tasks. \\n6. Improved performance on real-world scenarios through the development of a new high-quality human evaluation set, covering 12 key use cases such as asking for advice, brainstorming, and coding. \\n7. Enhanced capabilities in reasoning, code generation, and instruction following, making Llama3 more versatile'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                          chain_type = \"refine\",\n",
    "                                          retriever = retriever)\n",
    "\n",
    "refine_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The key improvements of Llama3 from its predecessors include a major leap in performance, reduced false refusal rates, improved alignment and diversity in model responses, and enhanced capabilities such as reasoning, code generation, and instruction following. Additionally, Llama3 will soon be available on various platforms and is expected to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance in the coming months.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                          chain_type = \"map_reduce\",\n",
    "                                          retriever = retriever)\n",
    "\n",
    "map_reduce_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(query):\n",
    "    chain1 = stuff_chain.run(query)\n",
    "    chain2 = refine_chain.run(query)\n",
    "    chain3 = map_reduce_chain.run(query)\n",
    "\n",
    "    return chain1, chain2, chain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arman\\OneDrive\\Documents\\GenAI\\RAG-demo\\FAISS\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def clear_textbox():\n",
    "    return \"\",\"\",\"\",\"\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Base(), title= \"Chain Types\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Comparing Different Chain Types\n",
    "        \"\"\")\n",
    "    \n",
    "    textbox = gr.Textbox(label=\"Enter query\")\n",
    "\n",
    "    with gr.Row():\n",
    "        button_send = gr.Button(\"Send\", variant=\"Primary\")\n",
    "        button_clear = gr.Button(\"Clear\", variant=\"Primary\")\n",
    "\n",
    "    with gr.Column():\n",
    "        output1 = gr.Textbox(lines=1, max_lines=15, label=\"Response from StuffDocumentChain: \")\n",
    "        output2 = gr.Textbox(lines=1, max_lines=15, label=\"Response from RefineDocumentChain: \")\n",
    "        output3 = gr.Textbox(lines=1, max_lines=15, label=\"Response from MapReduceDocumentChain: \")\n",
    "\n",
    "    button_send.click(query_data, textbox, outputs=[output1, output2, output3])\n",
    "    button_clear.click(clear_textbox, None, outputs=[textbox, output1, output2, output3])\n",
    "\n",
    "demo.launch()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
