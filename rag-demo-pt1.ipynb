{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Published', 'Title', 'Authors', 'Summary'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query=\"2405.17147\")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "docs[0].metadata.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='GPT in November 2022 after two months of which, it reached\\n100 million monthly users. Since then, many technical com-\\npanies started to build their open or closed foundation LLM\\nmodels, such as Gemini (Google), GPT-4 (OpenAI), LLaMA\\n(Meta), Claude (Anthropic), Falcon (TII), Mistral (Mixtral),\\netc. Initially, text-to-text is the conventional scenario of the\\nLLMs. All questions from users and replied answers from\\nLLMs are words. Starting from 2023, LLMs’ capability was\\nextended to multi-modality. In contrast to simple text-to-text\\nmode, the LLMs now support:\\n• Text-to-Image: Generate an image from text descriptions.\\n• Text-to-Video: Generate a video or movie from text\\ndescriptions, a bit like story-telling technologies.\\n• Text-to-Audio/Sound: Generate a human-like speech or\\nvarious environmental sounds from text descriptions.\\n• Image-to-Text: Summarize an image in a few sentences,\\nlike generating a caption of an image or photo.\\n• Video-to-Text: Summarize a video in a few sentences,', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='• Video-to-Text: Summarize a video in a few sentences,\\ne.g., creating a summary of a YouTube video clip or\\nNetflix movie.\\n• Audio/Sound-to-Text: Describe a speech or surrounding\\nsounds in a few sentences.\\nIf we consider 3D objects as a kind of modality, the list of\\nLLMs’ capability could even extended in the following:\\n• Text-to-3D: Generate realistic 3D objects from text de-\\nscriptions.\\n• Image-to-3D: Generate 3D scenes with an image or a 3D\\navatar with an portrait photo.\\n• Video-to-3D: Generate 3D scenes or eventually an entire\\n3D virtual environment by videos.\\nIt is noted that if it is from low-dimensional modality\\nto high-dimensional modality, it is a decoder/diffusion mode\\ndepicted as “generation”. Conversely, it is an encoder mode\\ndescribed as “summarization”.\\nThe use cases of LLMs are always as digital assistants in\\nconsumer electronics, such as smartphones, laptops, wearable\\ndevices, automotive, etc. For example, users speak to their', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='devices, automotive, etc. For example, users speak to their\\nsmartwatch about a query which is converted to text by\\nautomatic speech recognition (ASR) module, like Whisper.\\nThe LLMs running on central/fog/edge cloud answer the query\\nby considering the context of users (e.g., schedule, preference,\\netc.) with prompt engineering and finally provide a customized\\nnatural response by automatic text-to-speech conversion. For\\neXtended reality (including VR/AR/MR) use case [2], the\\nLLMs not only can generate human-like conversations for\\navatars, but also generate high-quality point clouds of the 3D\\nobjects and their corresponding textures. In general, in our\\nvision, we speculate that LLMs would replace the role of CPU\\nin our traditional computer architecture (shown in Fig. 1) and\\ninteract with web browsers, legacy components of PCs, local\\nfile systems, various multimedia modalities (including but not\\nlimited to images, audios, videos), and other LLM models.\\nFig. 1.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='limited to images, audios, videos), and other LLM models.\\nFig. 1.\\nLLMs will be the computational core to interact with multimodal\\nmultimedia, legacy file system, components of PC, and other LLMs.\\narXiv:2405.17147v1  [cs.MM]  27 May 2024\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n2\\nII. LLM DEPLOYMENT DEBATE: RAG VS FINE-TUNING\\nLLMs are highly effective in a wide range of natural\\nlanguage processing (NLP) tasks [3], but they often fall\\nshort in specific real-world applications due to a lack of\\nspecialized knowledge during their training. These models are\\ntypically trained on extensive, web-sourced annotated data,\\nwhich broadens their general applicability but diminishes their\\naccuracy in specialized areas. For instance, BloombergGPT\\n[4], which includes a significant proportion of finance-specific\\ndata, has shown improved performance in financial contexts,\\nillustrating the benefits of targeted data training. However,\\nassembling large, clean datasets for specialized training in-', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='assembling large, clean datasets for specialized training in-\\nvolves considerable logistical and cost challenges. In this\\ncase, deploying LLMs that have been trained primarily with\\ngeneric datasets can lead to challenges in processing long,\\ncomplex texts, thereby limiting their effectiveness in detailed\\ndomain discussions and increasing the likelihood of generating\\nhallucinations and producing irrelevant or incorrect content\\n[5]. To address this challenge, two main deployment strategies\\nhave emerged: Retrieval-Augmented Generation (RAG) and\\nfine-tuning.\\nA. RAG: Solving Context Window Limitation\\nThe RAG model enhances LLMs by integrating the external\\nknowledge base to efficiently retrieve relevant information that\\ncomplements the LLM’s inherent knowledge. This retrieved\\ninformation is then incorporated into the LLM’s prompt,\\nproviding essential contexts. The RAG mechanism is par-\\nticularly beneficial for tasks that require factual retrieval or', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='ticularly beneficial for tasks that require factual retrieval or\\nsimilarity searches, as it directly supplies relevant context,\\nthereby reducing the likelihood of generating inaccurate or hal-\\nlucinations. RAG can also be further enhanced by integrating\\ntechniques for in-context examples’ retrieval during runtime.\\nThis method involves fetching relevant few-shot examples that\\nare semantically similar to the user’s query.\\nAn illustration of a RAG-based LLM deployment system\\nis depicted in Fig. 2. This system utilizes a knowledge graph\\ndatabase to extract external knowledge and a vector database\\nfor retrieving relevant examples. To obtain reference context\\nfrom the knowledge graph database, the LLM first generates\\nCypher queries based on the user’s prompts and the fields\\nwithin the Resource Description Framework (RDF) knowledge\\ngraph. These queries are then executed in specific knowledge\\ngraph databases, such as Neo4j or SPARQL. The retrieved', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='graph databases, such as Neo4j or SPARQL. The retrieved\\ncontext may need to be re-ranked to ensure it aligns with\\nspecific business requirements. For retrieving few-shot exam-\\nples, which are represented as vectors in a high-dimensional\\nspace, the user’s query is transformed into the corresponding\\nembedding space. A similarity search is then performed to\\nselect the top-n semantically close examples. The final prompt\\nintegrates the aforementioned information with the initial user\\nprompt, enabling the LLM to generate responses that are both\\ninformative and accurate.\\nB. Fine-tuning: Deeper Domain Expertise\\nFine-tuning focuses on adapting a pre-trained LLM to a\\nspecific task through further supervised learning. This involves\\nUser Prompts\\nGenerated Answers\\nLarge Language Models\\nContext Window\\nKnowledge Base\\nKnowledge\\nGraph /\\nVector\\nDB\\nTask Description\\nReference Context\\nFew Shot Examples\\nVector\\nDB\\nReRank\\nRelevant Examples\\nUser Prompts\\nOutput Cue...\\nFig. 2.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='Few Shot Examples\\nVector\\nDB\\nReRank\\nRelevant Examples\\nUser Prompts\\nOutput Cue...\\nFig. 2.\\nRetrieval-Augmented Generation (RAG) architecture for large lan-\\nguage models. RAG retrieves relevant data from a knowledge base and\\noptionally few-shot examples to address context window limitations and\\nimprove response generation.\\nadjusting the weights of a pre-trained model on a smaller\\ndataset of task-specific examples. Compared to training a\\nmodel from scratch, fine-tuning offers significant advantages\\nin terms of computational efficiency and leveraging the pre-\\ntrained model’s general language understanding.\\nSeveral fine-tuning techniques have been developed to en-\\nhance LLM performance on specific tasks. These include\\ninstruction tuning which provides the LLM with explicit\\ninstructions or constraints alongside the training data. Low-\\nRank Adaptation (LoRA) which introduces a small adapter\\nmodule on top of the pre-trained LLM (shown in Fig. 3).', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='module on top of the pre-trained LLM (shown in Fig. 3).\\nThis adapter module is specifically trained on the task-specific\\ndata, enabling efficient adaptation without modifying the core\\nLLM architecture [6]. Recent advancements in fine-tuning\\nincorporate techniques from Reinforcement Learning from\\nHuman Feedback (RLHF) to achieve better alignment between\\nthe model’s outputs and human expectations [7]. RLHF allows\\nthe model to learn through interactions with a human in the\\nloop, who provides feedback on the model’s outputs. This\\nfeedback is then used to refine the model’s parameters and\\nimprove its performance on the target task.\\nPretrained\\nLLM\\nSmaller\\nTarget\\nDataset\\nFinetued\\nLLM\\nResponse A\\nResponse B\\nResponse C\\nResponse D\\nHuman Ranked\\nResponses\\nRM\\nTraining\\nRL\\nReward\\xa0\\nModel (RM)\\nReinforcement\\nLearning (RL)\\nModel\\nFig. 3. LLM fine-tuning for a specific task. The pre-trained model has learned\\nparameters from a massive dataset, while the parameters are updated through', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='parameters from a massive dataset, while the parameters are updated through\\nfine-tuning in a smaller target dataset. The red dots are the learned parameters\\nin the original models, while the green dots are the new parameters learned\\nfrom LoRA training. RLHF aligns the model for optimal performance.\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n3\\nC. Takeaway Message\\nThis section highlights the key considerations when choos-\\ning between RAG and fine-tuning for LLM applications.\\nRAG offers faster adaptation and leverages external knowledge\\nbases for factual retrieval and similarity search. However,\\nthe quality of the retrieved information heavily relies on\\nthe underlying knowledge base. If the knowledge base is\\noutdated, inaccurate, or incomplete, the retrieved information\\ncan mislead the LLM to generate factually incorrect outputs.\\nAdditionally, RAG struggles with tasks requiring complex\\nreasoning or inference beyond the information explicitly con-', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='reasoning or inference beyond the information explicitly con-\\ntained in the knowledge base [8]. Integrating and reasoning\\nover information from multiple sources can also be challenging\\nfor RAG models, potentially leading to inconsistencies or inac-\\ncuracies in the generated response. One approach to alleviate\\nthese limitations is to employ reranking techniques. Reranking\\ninvolves applying a secondary sorting step to the retrieved\\ninformation from the knowledge base. This step can be based\\non various factors, such as using a confidence score assigned\\nby the retrieval model [9], or evaluating whether the retrieved\\nitems collectively address all aspects of the user query.\\nFine-tuning, on the other hand, provides superior perfor-\\nmance for tasks requiring deep domain expertise [10]. How-\\never, it typically requires large, high-quality datasets to achieve\\noptimal performance whereas creating such datasets can be\\nexpensive and time-consuming. To address this challenge,', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='expensive and time-consuming. To address this challenge,\\nsome researchers are exploring the use of teacher-student\\nlearning. In this approach, a large, pre-trained model (teacher)\\nlike GPT4 [11] is used to generate synthetic data that can\\nbe used to fine-tune a smaller, open-source model (student)\\nlike LLaMA 8B [12]. This approach can be significantly\\ncheaper than creating high-quality human-annotated datasets,\\nespecially for tasks where large amounts of data are required.\\nHowever, it is important to ensure that the synthetic data\\naccurately reflects the real-world distribution of data and that\\nthe teacher model itself is not biased. Another challenge is the\\ncomputational cost of fine-tuning. Fine-tuning often requires\\nsignificant computational resources, especially for large LLMs.\\nThis can be a barrier for smaller organizations or researchers\\nwith limited access to computational resources.\\nThe choice between RAG and fine-tuning depends on the', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='The choice between RAG and fine-tuning depends on the\\nspecific needs of LLM applications. RAG offers a faster and\\nmore adaptable solution for retrieval-based tasks, while fine-\\ntuning provides superior performance for tasks demanding\\ndeeper domain expertise.\\nIII. TRAINING AND INFERENCE BY XPUS: CPU IS\\nOUT-OF-THE-DATE?\\nThe training of LLMs always consumes a lot of computa-\\ntional resources, particularly during the training of foundation\\nmodels. The referred computational resources herein are xPUs\\ntypically including GPUs (graphical processing units), TPUs\\n(tensor processing units), NPUs (neural processing units),\\nLPUs (language processing units), and CPUs (central process-\\ning units). The former four are designed for tensor processing\\n(i.e., vector manipulation and calculation) which are composed\\nof tens of thousands of cores. For instance, the newly-release\\nheavy-duty Blackwell GPU from Nvidia, GB202 GPU, has\\n24,576 CUDA cores. Even the light-duty Nvidia GeForce RTX', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='24,576 CUDA cores. Even the light-duty Nvidia GeForce RTX\\n4090 has 16,384 CUDA cores. In contrast, CPUs only have\\nvery few cores which are designed for sequential tasks rather\\nthan parallel matrix or vector calculations. For example, Intel\\ni9-14900 14th Gen has 24 cores.\\nThere is no doubt that GPUs/TPUs/NPUs/LPUs are the only\\noptions to train LLMs. In this case, thousands of xPU cards are\\noccupied to train an LLM for several months. Take the LLaMA\\n65B model as an example, it has 65 billion parameters. If\\neach parameter is represented by a floating point number,\\ni.e., 2 bytes to be stored, the total required storage would be\\n130 billion bytes, i.e., 130 gigabytes. As it is more than the\\nmemory limit of any single xPU card, the model parameters\\nneed to be distributed in different xPUs, named model paral-\\nlelism. To further improve the training efficiency and training\\ncompletion time, the training data is shared across xPU cards', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='completion time, the training data is shared across xPU cards\\ncalled data parallelism. Specifically, if we use Nvidia A100\\nGPU (80GB memory) cards to train the model, considering\\nthe overall dataset contains 1.4 trillion tokens (1 token is\\napproximately 0.75 English words), it takes 2048 A100 GPUs\\nfor approximately 21 days to complete the training. Even in the\\nfine-tuning phase where domain knowledge tokens are from a\\nfew thousand to tens of thousands, 8 GPU cards are always\\nat least required.\\nCertainly,\\nthe\\nLLM\\ninference\\ncan\\nbe\\ndone\\nby\\nGPUs/TPUs/NPUs/LPUs.\\nFor\\nsome\\nservice\\nproviders,\\nthe strategy is to use a big number of small xPU cards (in the\\nperspective of memory) to conduct inference. For example,\\nstill for LLaMA 65B model, if each parameter is stored in the\\nformat of int8 that is one byte, the total required storage is 65\\ngigabytes. If we use Groq LPU cards in this inference task,\\neach Groq card has 230 megabytes memory. Therefore, the', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='each Groq card has 230 megabytes memory. Therefore, the\\napproximate number of Groq cards for LLaMA 65B model\\ninference is (65×1000×1000×1000)÷(230×1000×1000) =\\n282.6 .\\n= 283.\\nHowever, these high-end xPU cards are not the only ones\\nthat fit the LLM inference scenarios. The main reason is that\\nthe inference models are always optimized to be much smaller\\nmodels in the perspective of parameter number. In addition,\\neach parameter can be used even less accurate format to\\nrepresent it. In that case, CPUs are also capable of conducting\\ninference without much loss of accuracy. Furthermore, CPUs\\nare the ideal hardware to fulfill the tasks of essential LLM pre-\\nprocessing, such as embedding and tokenization. Finally, the\\ninfrastructures of most existing data centers are composed of\\nCPU servers. By considering the expensive cost of upgrading\\nto high-end AI data centers, the service providers would prefer\\ncost-effective yet usable CPUs for the inference business.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='cost-effective yet usable CPUs for the inference business.\\nIV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\\nCOMPROMISE OF PERFORMANCE AND COST\\nTokenomics is a compound word for token and economics\\nreferring to the analysis of generative tokens in LLM infer-\\nence from the perspective of economics. Here, we usually\\nconsider two aspects: throughput (tokens per second) and\\nprice (USD per 1 million tokens). According to the up-to-\\ndate data [13], the throughput of most unicorn companies\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n4\\nFig. 4.\\nThe hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\\nbalancing factors, including inference accuracy, latency, device capacity, privacy, and security.\\n(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\\nFireworks, Groq, Leption) lies about 50 to 200 tokens per\\nsecond whereas Groq leads the benchmark to be more than\\n400 tokens per second. In regard to prices, most of the', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='400 tokens per second. In regard to prices, most of the\\naforementioned companies can achieve between $0.2 USD to\\n$1.0 USD per 1 million tokens. Perplexity and Groq lead the\\nboard herein around $0.25 USD.\\nIf we consider the LLM inference as a service, the cus-\\ntomers are users who ask questions by prompts and receive\\nanswers in a paid (e.g., OpenAI GPT-4) or unpaid way (e.g.,\\nOpenAI GPT-3.5 or GPT-4o). Each data center with thousands\\nof or even tens of thousands of AI servers is like a factory\\nwhose products are tokens. During this service, the metrics to\\nevaluate the quality of experience (QoE) of end users include\\n[14]:\\n• Time to first token (TTFT): The waiting time of the\\nusers to receive the response after entering their query.\\nTypically, one second of TTFT is considered to be good\\nenough.\\n• Time per output token (TPOT): The time to receive each\\ngenerated token by the users. If we consider a TPOT as\\n100 milliseconds per token, equivalent to 10 tokens per', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='100 milliseconds per token, equivalent to 10 tokens per\\nsecond, leading to 600 words per minute that is faster\\nthan most of the users can read.\\n• Latency:\\nThe\\noverall\\ntime\\nof\\nreceiving\\nan\\nan-\\nswer\\ncomposed\\nof\\ntokens\\nafter\\nthe\\ninitial\\nquery\\nfrom\\nthe\\nusers.\\nTo\\nbe\\nmore\\nprecise,\\nit\\ncan\\nbe\\ndescribed\\nas\\nLatency\\n=\\nTTFT\\n+ TOPT\\n∗\\n(number of total generated tokens).\\n• Throughput: The generated tokens per second. It can\\nbe considered as a median value in a time window\\nconsidering applicable queries and users.\\nIt is important for users to receive high QoE while using\\nthe LLM service. However, better service requires more com-\\nputational and networking resources. It is speculated that the\\nLLM service will be provided with different levels relating\\nto various subscription options or an one-time charge from\\nusers. It would be finally a compromise decision of users in\\nthe consideration of performance and cost.\\nV. HYBRID LLM: GRAVITATING TOWARDS THE EDGE', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='the consideration of performance and cost.\\nV. HYBRID LLM: GRAVITATING TOWARDS THE EDGE\\nIn our vision, the training of the LLMs happens in central\\nclouds or more specifically AI data centers that are composed\\nof training clusters, inference clusters, and dedicated storage,\\nas shown in Fig. 4. The models in the AI data centers are the\\nfull model without any approximation, such as weight pruning\\nand neural pruning. In order to reduce end-to-end latency and\\noperation costs, the AI processing of LLM inference would be\\nas close to the users as possible in a hierarchy, from central\\nclouds to edge clouds, until the devices. The potential devices\\nto provide LLM service include autonomous vehicles / IoVs\\n(Internet of Vehicles), IoT (Internet of Things), Drones, XR\\n(eXtended Reality) headsets [15], [16], Wearable devices (e.g.,\\nsmartwatch), robots (e.g., humanoid robot assistant) [17], etc.\\nDue to the computational capability of the devices, the LLM', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='Due to the computational capability of the devices, the LLM\\nmodels running on the device would be optimized: a much\\nsmaller model size (7 to 10 times) while keeping the accuracy\\nsatisfied. In practice, the LLMs on the device generate a few\\ntokens (typically four for example) as “drafts” which are then\\nsent to the cloud for checking the accuracy. Once confirmed or\\ncorrected by the cloud LLMs, the speculative decoding process\\nis complete and users receive the LLM answers.\\nIn regard to communication, the xPU servers are connected\\nby ToR (Top-of-Rack) Switches (electrical, optical, or hybrid)\\nwhich are further connected together according to a specific\\ntopology, such as fat-tree, dragonfly, etc. via InfiniBand or\\nEthernet. Communication between central clouds and fog/edge\\nclouds is WAN (Wide Area Network) which might also involve\\nDCI (Data Center Interconnect) if a training task is distributed\\nin multiple clouds. Between edge clouds and user devices, we', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='in multiple clouds. Between edge clouds and user devices, we\\nbelieve wireless 5G or 6G in the future would be the most\\nconvenient and efficient way. Due to cybersecurity factors, like\\nprivacy and security requirements [18], all packets need to go\\nthrough firewalls and be encrypted.\\nVI. CARBON FOOTPRINT AND SUSTAINABILITY OF LLMS\\nThe carbon footprint of a LLM comprises two fundamental\\ncomponents: the operational footprint and the embodied foot-\\nprint [19]. The operational footprint encompasses emissions\\nstemming from the energy consumption of the hardware used\\nduring pre-training, fine-tuning, and inference. The embodied\\nfootprint encapsulates the lifecycle emissions associated with\\nhardware manufacturing, including material extraction, pro-\\ncessing, and transportation. Accurately estimating the carbon\\nfootprint of LLMs before training is crucial for promoting\\nenvironmentally conscious development practices. This en-\\nables researchers and developers to make informed decisions', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='ables researchers and developers to make informed decisions\\nregarding model design and training procedures, promoting\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n5\\nthe development of more sustainable LLMs. Tools like mlco2\\n[20] offer a preliminary assessment based on GPU usage,\\nbut they often have limitations, such as being unable to\\naccount for dense or mixture-of-experts (MoE) architectures.\\nLLMCarbon [21], a recently introduced end-to-end carbon\\nfootprint projection model, addresses these shortcomings by\\nproviding more comprehensive and nuanced estimations for\\nvarious LLM architectures, including dense and MoE models.\\nFor instance, LLMCarbon estimates that training a GPT-3\\nmodel could generate around 553.87 tCO2eq (tonnes of CO2\\nequivalent), compared to actual data, the disparity is only\\n+0.32% with the actual emit is 536.69 tCO2eq. However,\\nthe training operational carbon footprint estimation made by\\nmlco2 is 69% higher than the actual, because mlco2 assumes', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='mlco2 is 69% higher than the actual, because mlco2 assumes\\nall devices consistently operate at the peak computing through-\\nput using the peak power.\\nThe sustainability of LLMs can be viewed through a\\ntwo-fold lens: economic and environmental. LLMs achieve\\neconomic sustainability if the value they generate for orga-\\nnizations (e.g., enhanced efficiency, and improved customer\\nservice) exceeds the costs incurred from training, inference,\\nand hardware maintenance. Environmental sustainability re-\\nquires a multifaceted approach, encompassing renewable en-\\nergy sources for data centers, energy-efficient model architec-\\ntures, and hardware designed for low-power AI workloads. By\\nprioritizing both economic and environmental considerations,\\nLLM development can become a powerful force for positive\\nchange, driving innovation while minimizing its environmental\\nimpact.\\nHaiwei Dong (haiwei.dong@ieee.org) is currently\\na Director and Principal Researcher with Huawei', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='Haiwei Dong (haiwei.dong@ieee.org) is currently\\na Director and Principal Researcher with Huawei\\nCanada, and an Adjunct Professor with the Univer-\\nsity of Ottawa. He was a Principal Engineer with\\nArtificial Intelligence Competency Center, Huawei\\nTechnologies Canada, Toronto, ON, Canada, a Re-\\nsearch Scientist with the University of Ottawa,\\nOttawa, ON, Canada, a Postdoctoral Fellow with\\nNew York University, New York City, NY, USA, a\\nResearch Associate with the University of Toronto,\\nToronto, ON, Canada, and a Research Fellow (PD)\\nwith the Japan Society for the Promotion of Science, Tokyo, Japan. He\\nreceived the Ph.D. degree from Kobe University, Kobe, Japan in 2010 and\\nthe M.Eng. degree from Shanghai Jiao Tong University, Shanghai, China,\\nin 2008. His research interests include artificial intelligence, multimedia,\\nmetaverse, and robotics. He also serves as a Column Editor of IEEE Multi-\\nmedia Magazine; an Associate Editor of ACM Transactions on Multimedia', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='media Magazine; an Associate Editor of ACM Transactions on Multimedia\\nComputing, Communications, and Applications; and an Associate Editor of\\nIEEE Consumer Electronics Magazine. He is a Senior Member of IEEE, a\\nSenior Member of ACM, and a registered Professional Engineer in Ontario.\\nShuang Xie (shuang.xie@ieee.org) is currently a se-\\nnior Machine Learning Engineer at Shopify, Canada.\\nHer research interests include artificial intelligence,\\nlarge language models, and computer vision. She\\nreceived the Master degree from the University of\\nOttawa, Ottawa, Canada, 2019. Prior to that, she\\nreceived the M.Eng. degree from Sichuan University,\\nSichuan, China, 2017. She is a Member of IEEE, and\\na Member of IEEE Women in Engineering.\\nREFERENCES\\n[1] OpenAI.\\n(2024).\\n[Online].\\nAvailable:\\nhttps://openai.com/index/\\nhello-gpt-4o/\\n[2] Z. Long, H. Dong, and A. El Saddik, “Interacting with New York\\ncity data by Hololens through remote rendering,” IEEE Consumer', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='city data by Hololens through remote rendering,” IEEE Consumer\\nElectronics Magazine, vol. 11, no. 5, pp. 64–72, 2022.\\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\\nJ. Zhang, Z. Dong et al., “A survey of large language models,” arXiv\\npreprint arXiv:2303.18223, 2023.\\n[4] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, and G. Mann, “BloombergGPT: A Large\\nLanguage Model for Finance,” arXiv preprint arXiv:2303.17564, 2023.\\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nNeural Information Processing Systems, vol. 30, pp. 5998–6008, 2017.\\n[6] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\\narXiv preprint arXiv:2106.09685, 2021.\\n[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in Neural\\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\\n[8] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language\\nmodels in retrieval-augmented generation,” in Proceedings of the AAAI\\nConference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754–\\n17 762.\\n[9] Z. Cao, W. Li, S. Li, and F. Wei, “Retrieve, rerank and rewrite:\\nSoft template based neural summarization,” in Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics, 2018,\\npp. 152–161.\\n[10] A. Balaguer, V. Benara, R. L. de Freitas Cunha, R. d. M. Estev˜\\nao Filho,\\nT. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O.\\nNunes et al., “Rag vs Fine-tuning: Pipelines, Tradeoffs, and a Case\\nStudy on Agriculture,” arXiv e-prints, pp. arXiv–2401, 2024.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='Study on Agriculture,” arXiv e-prints, pp. arXiv–2401, 2024.\\n[11] OpenAI.\\n(2023).\\n[Online].\\nAvailable:\\nhttps://openai.com/index/\\ngpt-4-research/\\n[12] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction tuning with\\nGPT-4,” arXiv preprint arXiv:2304.03277, 2023.\\n[13] D. Patel and D. Nishball. (2024) Groq inference tokenomics: Speed,\\nbut at what cost? [Online]. Available: https://www.semianalysis.com/p/\\ngroq-inference-tokenomics-speed-but\\n[14] M.\\nAgarwal,\\nA.\\nQureshi,\\nN.\\nSardana,\\nL.\\nLi,\\nJ.\\nQuevedo,\\nand D. Khudia. (2024) LLM inference performance engineering:\\nBest practices. [Online]. Available: https://www.databricks.com/blog/\\nllm-inference-performance-engineering-best-practices\\n[15] H. Dong and Y. Liu, “Metaverse meets consumer electronics,” IEEE\\nConsumer Electronics Magazine, vol. 12, no. 3, pp. 17–19, 2023.\\n[16] Z. Long, H. Dong, and A. El Saddik, “Human-centric resource allocation\\nfor the metaverse with multiaccess edge computing,” IEEE Internet of', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"}),\n",
       " Document(page_content='for the metaverse with multiaccess edge computing,” IEEE Internet of\\nThings Journal, vol. 10, no. 22, pp. 19 993–20 005, 2023.\\n[17] H. Dong, Y. Liu, T. Chu, and A. El Saddik, “Bringing robots home: The\\nrise of AI robots in consumer electronics,” IEEE Consumer Electronics\\nMagazine, vol. 13, 2024.\\n[18] “The future of AI is hybrid,” Qualcomm, Tech. Rep., 2023.\\n[19] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, D. Brooks,\\nand C.-J. Wu, “Chasing carbon: The elusive environmental footprint of\\ncomputing,” in Proceedings of the IEEE International Symposium on\\nHigh-Performance Computer Architecture.\\nIEEE, 2021, pp. 854–867.\\n[20] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quanti-\\nfying the carbon emissions of machine learning,” arXiv preprint\\narXiv:1910.09700, 2019.\\n[21] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and\\nL. Jiang, “LLMCarbon: Modeling the end-to-end carbon footprint of\\nlarge language models,” arXiv preprint arXiv:2309.14393, 2023.', metadata={'Published': '2024-05-27', 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability', 'Authors': 'Haiwei Dong, Shuang Xie', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\"})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_data = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap = 100).split_documents(docs)\n",
    "chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.control.pinecone.Pinecone at 0x297ccaa7ce0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = \"rag-pc-demo\"\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "    while not pc.describe_index(index_name).status[\"pinecone index is ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x00000297CCECBAA0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x00000297CCECB5C0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore.from_documents(chunk_data, embeddings, index_name = index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ables researchers and developers to make informed decisions\\nregarding model design and training procedures, promoting\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n5\\nthe development of more sustainable LLMs. Tools like mlco2\\n[20] offer a preliminary assessment based on GPU usage,\\nbut they often have limitations, such as being unable to\\naccount for dense or mixture-of-experts (MoE) architectures.\\nLLMCarbon [21], a recently introduced end-to-end carbon\\nfootprint projection model, addresses these shortcomings by\\nproviding more comprehensive and nuanced estimations for\\nvarious LLM architectures, including dense and MoE models.\\nFor instance, LLMCarbon estimates that training a GPT-3\\nmodel could generate around 553.87 tCO2eq (tonnes of CO2\\nequivalent), compared to actual data, the disparity is only\\n+0.32% with the actual emit is 536.69 tCO2eq. However,\\nthe training operational carbon footprint estimation made by\\nmlco2 is 69% higher than the actual, because mlco2 assumes', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       " Document(page_content='in multiple clouds. Between edge clouds and user devices, we\\nbelieve wireless 5G or 6G in the future would be the most\\nconvenient and efficient way. Due to cybersecurity factors, like\\nprivacy and security requirements [18], all packets need to go\\nthrough firewalls and be encrypted.\\nVI. CARBON FOOTPRINT AND SUSTAINABILITY OF LLMS\\nThe carbon footprint of a LLM comprises two fundamental\\ncomponents: the operational footprint and the embodied foot-\\nprint [19]. The operational footprint encompasses emissions\\nstemming from the energy consumption of the hardware used\\nduring pre-training, fine-tuning, and inference. The embodied\\nfootprint encapsulates the lifecycle emissions associated with\\nhardware manufacturing, including material extraction, pro-\\ncessing, and transportation. Accurately estimating the carbon\\nfootprint of LLMs before training is crucial for promoting\\nenvironmentally conscious development practices. This en-\\nables researchers and developers to make informed decisions', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       " Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a retriever\n",
    "\n",
    "query = \"describe the carbon footprint by gpt 4o.\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs ={\"k\":3})\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='ables researchers and developers to make informed decisions\\nregarding model design and training procedures, promoting\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n5\\nthe development of more sustainable LLMs. Tools like mlco2\\n[20] offer a preliminary assessment based on GPU usage,\\nbut they often have limitations, such as being unable to\\naccount for dense or mixture-of-experts (MoE) architectures.\\nLLMCarbon [21], a recently introduced end-to-end carbon\\nfootprint projection model, addresses these shortcomings by\\nproviding more comprehensive and nuanced estimations for\\nvarious LLM architectures, including dense and MoE models.\\nFor instance, LLMCarbon estimates that training a GPT-3\\nmodel could generate around 553.87 tCO2eq (tonnes of CO2\\nequivalent), compared to actual data, the disparity is only\\n+0.32% with the actual emit is 536.69 tCO2eq. However,\\nthe training operational carbon footprint estimation made by\\nmlco2 is 69% higher than the actual, because mlco2 assumes', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  0.908989966),\n",
       " (Document(page_content='in multiple clouds. Between edge clouds and user devices, we\\nbelieve wireless 5G or 6G in the future would be the most\\nconvenient and efficient way. Due to cybersecurity factors, like\\nprivacy and security requirements [18], all packets need to go\\nthrough firewalls and be encrypted.\\nVI. CARBON FOOTPRINT AND SUSTAINABILITY OF LLMS\\nThe carbon footprint of a LLM comprises two fundamental\\ncomponents: the operational footprint and the embodied foot-\\nprint [19]. The operational footprint encompasses emissions\\nstemming from the energy consumption of the hardware used\\nduring pre-training, fine-tuning, and inference. The embodied\\nfootprint encapsulates the lifecycle emissions associated with\\nhardware manufacturing, including material extraction, pro-\\ncessing, and transportation. Accurately estimating the carbon\\nfootprint of LLMs before training is crucial for promoting\\nenvironmentally conscious development practices. This en-\\nables researchers and developers to make informed decisions', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  0.895809114),\n",
       " (Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  0.8904767035000001)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_and_score = vector_store._similarity_search_with_relevance_scores(query, k=3)\n",
    "doc_and_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ables researchers and developers to make informed decisions\n",
      "regarding model design and training procedures, promoting\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "5\n",
      "the development of more sustainable LLMs. Tools like mlco2\n",
      "[20] offer a preliminary assessment based on GPU usage,\n",
      "but they often have limitations, such as being unable to\n",
      "account for dense or mixture-of-experts (MoE) architectures.\n",
      "LLMCarbon [21], a recently introduced end-to-end carbon\n",
      "footprint projection model, addresses these shortcomings by\n",
      "providing more comprehensive and nuanced estimations for\n",
      "various LLM architectures, including dense and MoE models.\n",
      "For instance, LLMCarbon estimates that training a GPT-3\n",
      "model could generate around 553.87 tCO2eq (tonnes of CO2\n",
      "equivalent), compared to actual data, the disparity is only\n",
      "+0.32% with the actual emit is 536.69 tCO2eq. However,\n",
      "the training operational carbon footprint estimation made by\n",
      "mlco2 is 69% higher than the actual, because mlco2 assumes\n",
      "\n",
      "in multiple clouds. Between edge clouds and user devices, we\n",
      "believe wireless 5G or 6G in the future would be the most\n",
      "convenient and efficient way. Due to cybersecurity factors, like\n",
      "privacy and security requirements [18], all packets need to go\n",
      "through firewalls and be encrypted.\n",
      "VI. CARBON FOOTPRINT AND SUSTAINABILITY OF LLMS\n",
      "The carbon footprint of a LLM comprises two fundamental\n",
      "components: the operational footprint and the embodied foot-\n",
      "print [19]. The operational footprint encompasses emissions\n",
      "stemming from the energy consumption of the hardware used\n",
      "during pre-training, fine-tuning, and inference. The embodied\n",
      "footprint encapsulates the lifecycle emissions associated with\n",
      "hardware manufacturing, including material extraction, pro-\n",
      "cessing, and transportation. Accurately estimating the carbon\n",
      "footprint of LLMs before training is crucial for promoting\n",
      "environmentally conscious development practices. This en-\n",
      "ables researchers and developers to make informed decisions\n",
      "\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "Lastly, we envisioned the future hybrid architecture of LLM\n",
      "processing and its corresponding sustainability concerns, partic-\n",
      "ularly in the environmental carbon footprint impact. Through\n",
      "these discussions, we provided a comprehensive overview of\n",
      "the operational and strategic considerations essential for the\n",
      "responsible development and deployment of LLMs.\n",
      "I. UBIQUITOUS LLMS\n",
      "The recent unveiling of GPT-4o by OpenAI on May 13,\n",
      "2024 marks a pivotal moment in the evolution of large\n",
      "language models (LLMs) [1]. This groundbreaking model,\n",
      "aptly named with “o” signifying “omni” for its comprehensive\n",
      "capabilities, transcends the limitations of its predecessors by\n",
      "incorporating multi modality. This signifies a significant step\n",
      "towards achieving more natural and intuitive human-computer\n",
      "interaction.\n",
      "The emergence of LLMs started from the launch of Chat-\n",
      "GPT in November 2022 after two months of which, it reached\n"
     ]
    }
   ],
   "source": [
    "query = \"describe the carbon footprint by gpt 4o.\"\n",
    "\n",
    "retrieved_docs =retriever.invoke(query)\n",
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prompt\n",
    "\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "# Question: {question} \n",
    "# Context: {context} \n",
    "# Answer:\n",
    "\n",
    "question = \"What are the primary advantages of using Retrieval-Augmented Generation (RAG)?\"\n",
    "context = \"RAG combines retrieval mechanisms with generative models to provide accurate and relevant responses by fetching information from external sources and generating coherent answers.\"\n",
    "\n",
    "# Format the prompt with the given question and context\n",
    "prompt = prompt_template.format(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a customised prompt\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The expanded functionalities of GPT-4o, such as multi-modality capabilities, could lead to a wide range of potential applications. Some potential applications could include: \\n1. Improved human-computer interaction: With the ability to generate images, videos, and audio based on text descriptions, GPT-4o could greatly enhance the way humans interact with computers, making it more natural and intuitive.\\n2. Content creation and storytelling: The text-to-video and text-to-audio functionalities could be utilized for creating content such as videos and audiobooks from text descriptions, making it easier for content creators to produce high-quality and engaging content.\\n3. Image and video summarization: The image-to-text and video-to-text functionalities could be used to automatically summarize images and videos, making it easier for users to quickly understand and process visual information.\\n4. Virtual assistants and chatbots: GPT-4o's expanded functionalities could greatly improve the capabilities of virtual assistants and chatbots, making them more human-like and capable of understanding and responding to a wider range of user inputs.\\n5. Accessibility tools: The text-to-sound functionality could be used to create human-like speech from text descriptions, making it easier for people with visual impairments to access written content.\\nOverall, the expanded\""
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what potential applications could arise from gpt-4o expanded functionalities?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_response(message,  memory):\n",
    "    return rag_chain.invoke(message)\n",
    "\n",
    "# Define the button instances with labels\n",
    "retry_button = gr.Button(\"Retry\")\n",
    "undo_button = gr.Button(\"Undo\")\n",
    "clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "# Pass the button instances to the ChatInterface\n",
    "rag_demo = gr.ChatInterface(\n",
    "                chat_response,\n",
    "                chatbot=gr.Chatbot(height=300),\n",
    "                textbox=gr.Textbox(placeholder=\"Enter query here\", container=False, scale=7),\n",
    "                title=\"RAG Demo\",\n",
    "                examples=[\"How does the introduction of GPT-4o by OpenAI, with its multimodal capabilities, represent a pivotal advancement in the evolution of large language models, and what potential applications could arise from these expanded functionalities?\", \"What are the primary advantages and challenges of deploying large language models using Retrieval-Augmented Generation (RAG) compared to fine-tuning, particularly in terms of handling specialized knowledge and reducing hallucinations?\"],\n",
    "                cache_examples=False,\n",
    "                retry_btn=retry_button,\n",
    "                undo_btn=undo_button,\n",
    "                clear_btn=clear_button,\n",
    "                submit_btn=\"Submit\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
