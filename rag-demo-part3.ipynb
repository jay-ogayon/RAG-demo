{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader=ArxivLoader(query=\"2405.17147\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chunk data\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap =100)\n",
    "chunk_docs = text_splitter.split_documents(docs)\n",
    "len(chunk_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devrpa.operations\\Documents\\RAG\\RAG-Demo1\\.venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"pc-rag\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name= index_name,\n",
    "        dimension=1536,\n",
    "        metric = \"cosine\",\n",
    "        spec= ServerlessSpec(cloud=\"aws\", region= \"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsert data\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore.from_documents(chunk_docs, embeddings, index_name = index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devrpa.operations\\Documents\\RAG\\RAG-Demo1\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What metrics are used to evaluate the quality of experience (QoE) for users of large language model (LLM) services?\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs = {\"k\":3})\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(query):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
      "\n",
      "Guidelines:\n",
      "1. Provide concise and informative answers.\n",
      "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
      "3. Use examples where applicable to illustrate your answers.\n",
      "4. Maintain a professional and helpful tone.\n",
      "\n",
      "Context: CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "1\n",
      "Large Language Models (LLMs):\n",
      "Deployment, Tokenomics and Sustainability\n",
      "Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\n",
      "Abstract—The rapid advancement of Large Language Models\n",
      "(LLMs) has significantly impacted human-computer interaction,\n",
      "epitomized by the release of GPT-4o, which introduced com-\n",
      "prehensive multi-modality capabilities. In this paper, we first\n",
      "explored the deployment strategies, economic considerations,\n",
      "and sustainability challenges associated with the state-of-the-art\n",
      "LLMs. More specifically, we discussed the deployment debate\n",
      "between Retrieval-Augmented Generation (RAG) and fine-tuning,\n",
      "highlighting their respective advantages and limitations. After\n",
      "that, we quantitatively analyzed the requirement of xPUs in\n",
      "training and inference. Additionally, for the tokenomics of LLM\n",
      "services, we examined the balance between performance and cost\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "Lastly, we envisioned the future hybrid architecture of LLM\n",
      "processing and its corresponding sustainability concerns, partic-\n",
      "ularly in the environmental carbon footprint impact. Through\n",
      "these discussions, we provided a comprehensive overview of\n",
      "the operational and strategic considerations essential for the\n",
      "responsible development and deployment of LLMs.\n",
      "I. UBIQUITOUS LLMS\n",
      "The recent unveiling of GPT-4o by OpenAI on May 13,\n",
      "2024 marks a pivotal moment in the evolution of large\n",
      "language models (LLMs) [1]. This groundbreaking model,\n",
      "aptly named with “o” signifying “omni” for its comprehensive\n",
      "capabilities, transcends the limitations of its predecessors by\n",
      "incorporating multi modality. This signifies a significant step\n",
      "towards achieving more natural and intuitive human-computer\n",
      "interaction.\n",
      "The emergence of LLMs started from the launch of Chat-\n",
      "GPT in November 2022 after two months of which, it reached\n",
      "100 million monthly users. Since then, many technical com-\n",
      "panies started to build their open or closed foundation LLM\n",
      "models, such as Gemini (Google), GPT-4 (OpenAI), LLaMA\n",
      "(Meta), Claude (Anthropic), Falcon (TII), Mistral (Mixtral),\n",
      "etc. Initially, text-to-text is the conventional scenario of the\n",
      "LLMs. All questions from users and replied answers from\n",
      "LLMs are words. Starting from 2023, LLMs’ capability was\n",
      "extended to multi-modality. In contrast to simple text-to-text\n",
      "mode, the LLMs now support:\n",
      "• Text-to-Image: Generate an image from text descriptions.\n",
      "• Text-to-Video: Generate a video or movie from text\n",
      "descriptions, a bit like story-telling technologies.\n",
      "• Text-to-Audio/Sound: Generate a human-like speech or\n",
      "various environmental sounds from text descriptions.\n",
      "• Image-to-Text: Summarize an image in a few sentences,\n",
      "like generating a caption of an image or photo.\n",
      "• Video-to-Text: Summarize a video in a few sentences,\n",
      "e.g., creating a summary of a YouTube video clip or\n",
      "Netflix movie.\n",
      "• Audio/Sound-to-Text: Describe a speech or surrounding\n",
      "sounds in a few sentences.\n",
      "If we consider 3D objects as a kind of modality, the list of\n",
      "LLMs’ capability could even extended in the following:\n",
      "• Text-to-3D: Generate realistic 3D objects from text de-\n",
      "scriptions.\n",
      "• Image-to-3D: Generate 3D scenes with an image or a 3D\n",
      "avatar with an portrait photo.\n",
      "• Video-to-3D: Generate 3D scenes or eventually an entire\n",
      "3D virtual environment by videos.\n",
      "It is noted that if it is from low-dimensional modality\n",
      "to high-dimensional modality, it is a decoder/diffusion mode\n",
      "depicted as “generation”. Conversely, it is an encoder mode\n",
      "described as “summarization”.\n",
      "The use cases of LLMs are always as digital assistants in\n",
      "consumer electronics, such as smartphones, laptops, wearable\n",
      "devices, automotive, etc. For example, users speak to their\n",
      "smartwatch about a query which is converted to text by\n",
      "automatic speech recognition (ASR) module, like Whisper.\n",
      "The LLMs running on central/fog/edge cloud answer the query\n",
      "by considering the context of users (e.g., schedule, preference,\n",
      "etc.) with prompt engineering and finally provide a customized\n",
      "natural response by automatic text-to-speech conversion. For\n",
      "eXtended reality (including VR/AR/MR) use case [2], the\n",
      "LLMs not only can generate human-like conversations for\n",
      "avatars, but also generate high-quality point clouds of the 3D\n",
      "objects and their corresponding textures. In general, in our\n",
      "vision, we speculate that LLMs would replace the role of CPU\n",
      "in our traditional computer architecture (shown in Fig. 1) and\n",
      "interact with web browsers, legacy components of PCs, local\n",
      "file systems, various multimedia modalities (including but not\n",
      "limited to images, audios, videos), and other LLM models.\n",
      "Fig. 1.\n",
      "LLMs will be the computational core to interact with multimodal\n",
      "multimedia, legacy file system, components of PC, and other LLMs.\n",
      "arXiv:2405.17147v1  [cs.MM]  27 May 2024\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "2\n",
      "II. LLM DEPLOYMENT DEBATE: RAG VS FINE-TUNING\n",
      "LLMs are highly effective in a wide range of natural\n",
      "language processing (NLP) tasks [3], but they often fall\n",
      "short in specific real-world applications due to a lack of\n",
      "specialized knowledge during their training. These models are\n",
      "typically trained on extensive, web-sourced annotated data,\n",
      "which broadens their general applicability but diminishes their\n",
      "accuracy in specialized areas. For instance, BloombergGPT\n",
      "[4], which includes a significant proportion of finance-specific\n",
      "data, has shown improved performance in financial contexts,\n",
      "illustrating the benefits of targeted data training. However,\n",
      "assembling large, clean datasets for specialized training in-\n",
      "volves considerable logistical and cost challenges. In this\n",
      "case, deploying LLMs that have been trained primarily with\n",
      "generic datasets can lead to challenges in processing long,\n",
      "complex texts, thereby limiting their effectiveness in detailed\n",
      "domain discussions and increasing the likelihood of generating\n",
      "hallucinations and producing irrelevant or incorrect content\n",
      "[5]. To address this challenge, two main deployment strategies\n",
      "have emerged: Retrieval-Augmented Generation (RAG) and\n",
      "fine-tuning.\n",
      "A. RAG: Solving Context Window Limitation\n",
      "The RAG model enhances LLMs by integrating the external\n",
      "knowledge base to efficiently retrieve relevant information that\n",
      "complements the LLM’s inherent knowledge. This retrieved\n",
      "information is then incorporated into the LLM’s prompt,\n",
      "providing essential contexts. The RAG mechanism is par-\n",
      "ticularly beneficial for tasks that require factual retrieval or\n",
      "similarity searches, as it directly supplies relevant context,\n",
      "thereby reducing the likelihood of generating inaccurate or hal-\n",
      "lucinations. RAG can also be further enhanced by integrating\n",
      "techniques for in-context examples’ retrieval during runtime.\n",
      "This method involves fetching relevant few-shot examples that\n",
      "are semantically similar to the user’s query.\n",
      "An illustration of a RAG-based LLM deployment system\n",
      "is depicted in Fig. 2. This system utilizes a knowledge graph\n",
      "database to extract external knowledge and a vector database\n",
      "for retrieving relevant examples. To obtain reference context\n",
      "from the knowledge graph database, the LLM first generates\n",
      "Cypher queries based on the user’s prompts and the fields\n",
      "within the Resource Description Framework (RDF) knowledge\n",
      "graph. These queries are then executed in specific knowledge\n",
      "graph databases, such as Neo4j or SPARQL. The retrieved\n",
      "context may need to be re-ranked to ensure it aligns with\n",
      "specific business requirements. For retrieving few-shot exam-\n",
      "ples, which are represented as vectors in a high-dimensional\n",
      "space, the user’s query is transformed into the corresponding\n",
      "embedding space. A similarity search is then performed to\n",
      "select the top-n semantically close examples. The final prompt\n",
      "integrates the aforementioned information with the initial user\n",
      "prompt, enabling the LLM to generate responses that are both\n",
      "informative and accurate.\n",
      "B. Fine-tuning: Deeper Domain Expertise\n",
      "Fine-tuning focuses on adapting a pre-trained LLM to a\n",
      "specific task through further supervised learning. This involves\n",
      "User Prompts\n",
      "Generated Answers\n",
      "Large Language Models\n",
      "Context Window\n",
      "Knowledge Base\n",
      "Knowledge\n",
      "Graph /\n",
      "Vector\n",
      "DB\n",
      "Task Description\n",
      "Reference Context\n",
      "Few Shot Examples\n",
      "Vector\n",
      "DB\n",
      "ReRank\n",
      "Relevant Examples\n",
      "User Prompts\n",
      "Output Cue...\n",
      "Fig. 2.\n",
      "Retrieval-Augmented Generation (RAG) architecture for large lan-\n",
      "guage models. RAG retrieves relevant data from a knowledge base and\n",
      "optionally few-shot examples to address context window limitations and\n",
      "improve response generation.\n",
      "adjusting the weights of a pre-trained model on a smaller\n",
      "dataset of task-specific examples. Compared to training a\n",
      "model from scratch, fine-tuning offers significant advantages\n",
      "in terms of computational efficiency and leveraging the pre-\n",
      "trained model’s general language understanding.\n",
      "Several fine-tuning techniques have been developed to en-\n",
      "hance LLM performance on specific tasks. These include\n",
      "instruction tuning which provides the LLM with explicit\n",
      "instructions or constraints alongside the training data. Low-\n",
      "Rank Adaptation (LoRA) which introduces a small adapter\n",
      "module on top of the pre-trained LLM (shown in Fig. 3).\n",
      "This adapter module is specifically trained on the task-specific\n",
      "data, enabling efficient adaptation without modifying the core\n",
      "LLM architecture [6]. Recent advancements in fine-tuning\n",
      "incorporate techniques from Reinforcement Learning from\n",
      "Human Feedback (RLHF) to achieve better alignment between\n",
      "the model’s outputs and human expectations [7]. RLHF allows\n",
      "the model to learn through interactions with a human in the\n",
      "loop, who provides feedback on the model’s outputs. This\n",
      "feedback is then used to refine the model’s parameters and\n",
      "improve its performance on the target task.\n",
      "Pretrained\n",
      "LLM\n",
      "Smaller\n",
      "Target\n",
      "Dataset\n",
      "Finetued\n",
      "LLM\n",
      "Response A\n",
      "Response B\n",
      "Response C\n",
      "Response D\n",
      "Human Ranked\n",
      "Responses\n",
      "RM\n",
      "Training\n",
      "RL\n",
      "Reward \n",
      "Model (RM)\n",
      "Reinforcement\n",
      "Learning (RL)\n",
      "Model\n",
      "Fig. 3. LLM fine-tuning for a specific task. The pre-trained model has learned\n",
      "parameters from a massive dataset, while the parameters are updated through\n",
      "fine-tuning in a smaller target dataset. The red dots are the learned parameters\n",
      "in the original models, while the green dots are the new parameters learned\n",
      "from LoRA training. RLHF aligns the model for optimal performance.\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "3\n",
      "C. Takeaway Message\n",
      "This section highlights the key considerations when choos-\n",
      "ing between RAG and fine-tuning for LLM applications.\n",
      "RAG offers faster adaptation and leverages external knowledge\n",
      "bases for factual retrieval and similarity search. However,\n",
      "the quality of the retrieved information heavily relies on\n",
      "the underlying knowledge base. If the knowledge base is\n",
      "outdated, inaccurate, or incomplete, the retrieved information\n",
      "can mislead the LLM to generate factually incorrect outputs.\n",
      "Additionally, RAG struggles with tasks requiring complex\n",
      "reasoning or inference beyond the information explicitly con-\n",
      "tained in the knowledge base [8]. Integrating and reasoning\n",
      "over information from multiple sources can also be challenging\n",
      "for RAG models, potentially leading to inconsistencies or inac-\n",
      "curacies in the generated response. One approach to alleviate\n",
      "these limitations is to employ reranking techniques. Reranking\n",
      "involves applying a secondary sorting step to the retrieved\n",
      "information from the knowledge base. This step can be based\n",
      "on various factors, such as using a confidence score assigned\n",
      "by the retrieval model [9], or evaluating whether the retrieved\n",
      "items collectively address all aspects of the user query.\n",
      "Fine-tuning, on the other hand, provides superior perfor-\n",
      "mance for tasks requiring deep domain expertise [10]. How-\n",
      "ever, it typically requires large, high-quality datasets to achieve\n",
      "optimal performance whereas creating such datasets can be\n",
      "expensive and time-consuming. To address this challenge,\n",
      "some researchers are exploring the use of teacher-student\n",
      "learning. In this approach, a large, pre-trained model (teacher)\n",
      "like GPT4 [11] is used to generate synthetic data that can\n",
      "be used to fine-tune a smaller, open-source model (student)\n",
      "like LLaMA 8B [12]. This approach can be significantly\n",
      "cheaper than creating high-quality human-annotated datasets,\n",
      "especially for tasks where large amounts of data are required.\n",
      "However, it is important to ensure that the synthetic data\n",
      "accurately reflects the real-world distribution of data and that\n",
      "the teacher model itself is not biased. Another challenge is the\n",
      "computational cost of fine-tuning. Fine-tuning often requires\n",
      "significant computational resources, especially for large LLMs.\n",
      "This can be a barrier for smaller organizations or researchers\n",
      "with limited access to computational resources.\n",
      "The choice between RAG and fine-tuning depends on the\n",
      "specific needs of LLM applications. RAG offers a faster and\n",
      "more adaptable solution for retrieval-based tasks, while fine-\n",
      "tuning provides superior performance for tasks demanding\n",
      "deeper domain expertise.\n",
      "III. TRAINING AND INFERENCE BY XPUS: CPU IS\n",
      "OUT-OF-THE-DATE?\n",
      "The training of LLMs always consumes a lot of computa-\n",
      "tional resources, particularly during the training of foundation\n",
      "models. The referred computational resources herein are xPUs\n",
      "typically including GPUs (graphical processing units), TPUs\n",
      "(tensor processing units), NPUs (neural processing units),\n",
      "LPUs (language processing units), and CPUs (central process-\n",
      "ing units). The former four are designed for tensor processing\n",
      "(i.e., vector manipulation and calculation) which are composed\n",
      "of tens of thousands of cores. For instance, the newly-release\n",
      "heavy-duty Blackwell GPU from Nvidia, GB202 GPU, has\n",
      "24,576 CUDA cores. Even the light-duty Nvidia GeForce RTX\n",
      "4090 has 16,384 CUDA cores. In contrast, CPUs only have\n",
      "very few cores which are designed for sequential tasks rather\n",
      "than parallel matrix or vector calculations. For example, Intel\n",
      "i9-14900 14th Gen has 24 cores.\n",
      "There is no doubt that GPUs/TPUs/NPUs/LPUs are the only\n",
      "options to train LLMs. In this case, thousands of xPU cards are\n",
      "occupied to train an LLM for several months. Take the LLaMA\n",
      "65B model as an example, it has 65 billion parameters. If\n",
      "each parameter is represented by a floating point number,\n",
      "i.e., 2 bytes to be stored, the total required storage would be\n",
      "130 billion bytes, i.e., 130 gigabytes. As it is more than the\n",
      "memory limit of any single xPU card, the model parameters\n",
      "need to be distributed in different xPUs, named model paral-\n",
      "lelism. To further improve the training efficiency and training\n",
      "completion time, the training data is shared across xPU cards\n",
      "called data parallelism. Specifically, if we use Nvidia A100\n",
      "GPU (80GB memory) cards to train the model, considering\n",
      "the overall dataset contains 1.4 trillion tokens (1 token is\n",
      "approximately 0.75 English words), it takes 2048 A100 GPUs\n",
      "for approximately 21 days to complete the training. Even in the\n",
      "fine-tuning phase where domain knowledge tokens are from a\n",
      "few thousand to tens of thousands, 8 GPU cards are always\n",
      "at least required.\n",
      "Certainly,\n",
      "the\n",
      "LLM\n",
      "inference\n",
      "can\n",
      "be\n",
      "done\n",
      "by\n",
      "GPUs/TPUs/NPUs/LPUs.\n",
      "For\n",
      "some\n",
      "service\n",
      "providers,\n",
      "the strategy is to use a big number of small xPU cards (in the\n",
      "perspective of memory) to conduct inference. For example,\n",
      "still for LLaMA 65B model, if each parameter is stored in the\n",
      "format of int8 that is one byte, the total required storage is 65\n",
      "gigabytes. If we use Groq LPU cards in this inference task,\n",
      "each Groq card has 230 megabytes memory. Therefore, the\n",
      "approximate number of Groq cards for LLaMA 65B model\n",
      "inference is (65×1000×1000×1000)÷(230×1000×1000) =\n",
      "282.6 .\n",
      "= 283.\n",
      "However, these high-end xPU cards are not the only ones\n",
      "that fit the LLM inference scenarios. The main reason is that\n",
      "the inference models are always optimized to be much smaller\n",
      "models in the perspective of parameter number. In addition,\n",
      "each parameter can be used even less accurate format to\n",
      "represent it. In that case, CPUs are also capable of conducting\n",
      "inference without much loss of accuracy. Furthermore, CPUs\n",
      "are the ideal hardware to fulfill the tasks of essential LLM pre-\n",
      "processing, such as embedding and tokenization. Finally, the\n",
      "infrastructures of most existing data centers are composed of\n",
      "CPU servers. By considering the expensive cost of upgrading\n",
      "to high-end AI data centers, the service providers would prefer\n",
      "cost-effective yet usable CPUs for the inference business.\n",
      "IV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\n",
      "COMPROMISE OF PERFORMANCE AND COST\n",
      "Tokenomics is a compound word for token and economics\n",
      "referring to the analysis of generative tokens in LLM infer-\n",
      "ence from the perspective of economics. Here, we usually\n",
      "consider two aspects: throughput (tokens per second) and\n",
      "price (USD per 1 million tokens). According to the up-to-\n",
      "date data [13], the throughput of most unicorn companies\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "4\n",
      "Fig. 4.\n",
      "The hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\n",
      "balancing factors, including inference accuracy, latency, device capacity, privacy, and security.\n",
      "(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\n",
      "Fireworks, Groq, Leption) lies about 50 to 200 tokens per\n",
      "second whereas Groq leads the benchmark to be more than\n",
      "400 tokens per second. In regard to prices, most of the\n",
      "aforementioned companies can achieve between $0.2 USD to\n",
      "$1.0 USD per 1 million tokens. Perplexity and Groq lead the\n",
      "board herein around $0.25 USD.\n",
      "If we consider the LLM inference as a service, the cus-\n",
      "tomers are users who ask questions by prompts and receive\n",
      "answers in a paid (e.g., OpenAI GPT-4) or unpaid way (e.g.,\n",
      "OpenAI GPT-3.5 or GPT-4o). Each data center with thousands\n",
      "of or even tens of thousands of AI servers is like a factory\n",
      "whose products are tokens. During this service, the metrics to\n",
      "evaluate the quality of experience (QoE) of end users include\n",
      "[14]:\n",
      "• Time to first token (TTFT): The waiting time of the\n",
      "users to receive the response after entering their query.\n",
      "Typically, one second of TTFT is considered to be good\n",
      "enough.\n",
      "• Time per output token (TPOT): The time to receive each\n",
      "generated token by the users. If we consider a TPOT as\n",
      "100 milliseconds per token, equivalent to 10 tokens per\n",
      "second, leading to 600 words per minute that is faster\n",
      "than most of the users can read.\n",
      "• Latency:\n",
      "The\n",
      "overall\n",
      "time\n",
      "of\n",
      "receiving\n",
      "an\n",
      "an-\n",
      "swer\n",
      "composed\n",
      "of\n",
      "tokens\n",
      "after\n",
      "the\n",
      "initial\n",
      "query\n",
      "from\n",
      "the\n",
      "users.\n",
      "To\n",
      "be\n",
      "more\n",
      "precise,\n",
      "it\n",
      "can\n",
      "be\n",
      "described\n",
      "as\n",
      "Latency\n",
      "=\n",
      "TTFT\n",
      "+ TOPT\n",
      "∗\n",
      "(number of total generated tokens).\n",
      "• Throughput: The generated tokens per second. It can\n",
      "be considered as a median value in a time window\n",
      "considering applicable queries and users.\n",
      "It is important for users to receive high QoE while using\n",
      "the LLM service. However, better service requires more com-\n",
      "putational and networking resources. It is speculated that the\n",
      "LLM service will be provided with different levels relating\n",
      "to various subscription options or an one-time charge from\n",
      "users. It would be finally a compromise decision of users in\n",
      "the consideration of performance and cost.\n",
      "V. HYBRID LLM: GRAVITATING TOWARDS THE EDGE\n",
      "In our vision, the training of the LLMs happens in central\n",
      "clouds or more specifically AI data centers that are composed\n",
      "of training clusters, inference clusters, and dedicated storage,\n",
      "as shown in Fig. 4. The models in the AI data centers are the\n",
      "full model without any approximation, such as weight pruning\n",
      "and neural pruning. In order to reduce end-to-end latency and\n",
      "operation costs, the AI processing of LLM inference would be\n",
      "as close to the users as possible in a hierarchy, from central\n",
      "clouds to edge clouds, until the devices. The potential devices\n",
      "to provide LLM service include autonomous vehicles / IoVs\n",
      "(Internet of Vehicles), IoT (Internet of Things), Drones, XR\n",
      "(eXtended Reality) headsets [15], [16], Wearable devices (e.g.,\n",
      "smartwatch), robots (e.g., humanoid robot assistant) [17], etc.\n",
      "Due to the computational capability of the devices, the LLM\n",
      "models running on the device would be optimized: a much\n",
      "smaller model size (7 to 10 times) while keeping the accuracy\n",
      "satisfied. In practice, the LLMs on the device generate a few\n",
      "tokens (typically four for example) as “drafts” which are then\n",
      "sent to the cloud for checking the accuracy. Once confirmed or\n",
      "corrected by the cloud LLMs, the speculative decoding process\n",
      "is complete and users receive the LLM answers.\n",
      "In regard to communication, the xPU servers are connected\n",
      "by ToR (Top-of-Rack) Switches (electrical, optical, or hybrid)\n",
      "which are further connected together according to a specific\n",
      "topology, such as fat-tree, dragonfly, etc. via InfiniBand or\n",
      "Ethernet. Communication between central clouds and fog/edge\n",
      "clouds is WAN (Wide Area Network) which might also involve\n",
      "DCI (Data Center Interconnect) if a training task is distributed\n",
      "in multiple clouds. Between edge clouds and user devices, we\n",
      "believe wireless 5G or 6G in the future would be the most\n",
      "convenient and efficient way. Due to cybersecurity factors, like\n",
      "privacy and security requirements [18], all packets need to go\n",
      "through firewalls and be encrypted.\n",
      "VI. CARBON FOOTPRINT AND SUSTAINABILITY OF LLMS\n",
      "The carbon footprint of a LLM comprises two fundamental\n",
      "components: the operational footprint and the embodied foot-\n",
      "print [19]. The operational footprint encompasses emissions\n",
      "stemming from the energy consumption of the hardware used\n",
      "during pre-training, fine-tuning, and inference. The embodied\n",
      "footprint encapsulates the lifecycle emissions associated with\n",
      "hardware manufacturing, including material extraction, pro-\n",
      "cessing, and transportation. Accurately estimating the carbon\n",
      "footprint of LLMs before training is crucial for promoting\n",
      "environmentally conscious development practices. This en-\n",
      "ables researchers and developers to make informed decisions\n",
      "regarding model design and training procedures, promoting\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "5\n",
      "the development of more sustainable LLMs. Tools like mlco2\n",
      "[20] offer a preliminary assessment based on GPU usage,\n",
      "but they often have limitations, such as being unable to\n",
      "account for dense or mixture-of-experts (MoE) architectures.\n",
      "LLMCarbon [21], a recently introduced end-to-end carbon\n",
      "footprint projection model, addresses these shortcomings by\n",
      "providing more comprehensive and nuanced estimations for\n",
      "various LLM architectures, including dense and MoE models.\n",
      "For instance, LLMCarbon estimates that training a GPT-3\n",
      "model could generate around 553.87 tCO2eq (tonnes of CO2\n",
      "equivalent), compared to actual data, the disparity is only\n",
      "+0.32% with the actual emit is 536.69 tCO2eq. However,\n",
      "the training operational carbon footprint estimation made by\n",
      "mlco2 is 69% higher than the actual, because mlco2 assumes\n",
      "all devices consistently operate at the peak computing through-\n",
      "put using the peak power.\n",
      "The sustainability of LLMs can be viewed through a\n",
      "two-fold lens: economic and environmental. LLMs achieve\n",
      "economic sustainability if the value they generate for orga-\n",
      "nizations (e.g., enhanced efficiency, and improved customer\n",
      "service) exceeds the costs incurred from training, inference,\n",
      "and hardware maintenance. Environmental sustainability re-\n",
      "quires a multifaceted approach, encompassing renewable en-\n",
      "ergy sources for data centers, energy-efficient model architec-\n",
      "tures, and hardware designed for low-power AI workloads. By\n",
      "prioritizing both economic and environmental considerations,\n",
      "LLM development can become a powerful force for positive\n",
      "change, driving innovation while minimizing its environmental\n",
      "impact.\n",
      "Haiwei Dong (haiwei.dong@ieee.org) is currently\n",
      "a Director and Principal Researcher with Huawei\n",
      "Canada, and an Adjunct Professor with the Univer-\n",
      "sity of Ottawa. He was a Principal Engineer with\n",
      "Artificial Intelligence Competency Center, Huawei\n",
      "Technologies Canada, Toronto, ON, Canada, a Re-\n",
      "search Scientist with the University of Ottawa,\n",
      "Ottawa, ON, Canada, a Postdoctoral Fellow with\n",
      "New York University, New York City, NY, USA, a\n",
      "Research Associate with the University of Toronto,\n",
      "Toronto, ON, Canada, and a Research Fellow (PD)\n",
      "with the Japan Society for the Promotion of Science, Tokyo, Japan. He\n",
      "received the Ph.D. degree from Kobe University, Kobe, Japan in 2010 and\n",
      "the M.Eng. degree from Shanghai Jiao Tong University, Shanghai, China,\n",
      "in 2008. His research interests include artificial intelligence, multimedia,\n",
      "metaverse, and robotics. He also serves as a Column Editor of IEEE Multi-\n",
      "media Magazine; an Associate Editor of ACM Transactions on Multimedia\n",
      "Computing, Communications, and Applications; and an Associate Editor of\n",
      "IEEE Consumer Electronics Magazine. He is a Senior Member of IEEE, a\n",
      "Senior Member of ACM, and a registered Professional Engineer in Ontario.\n",
      "Shuang Xie (shuang.xie@ieee.org) is currently a se-\n",
      "nior Machine Learning Engineer at Shopify, Canada.\n",
      "Her research interests include artificial intelligence,\n",
      "large language models, and computer vision. She\n",
      "received the Master degree from the University of\n",
      "Ottawa, Ottawa, Canada, 2019. Prior to that, she\n",
      "received the M.Eng. degree from Sichuan University,\n",
      "Sichuan, China, 2017. She is a Member of IEEE, and\n",
      "a Member of IEEE Women in Engineering.\n",
      "REFERENCES\n",
      "[1] OpenAI.\n",
      "(2024).\n",
      "[Online].\n",
      "Available:\n",
      "https://openai.com/index/\n",
      "hello-gpt-4o/\n",
      "[2] Z. Long, H. Dong, and A. El Saddik, “Interacting with New York\n",
      "city data by Hololens through remote rendering,” IEEE Consumer\n",
      "Electronics Magazine, vol. 11, no. 5, pp. 64–72, 2022.\n",
      "[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\n",
      "J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv\n",
      "preprint arXiv:2303.18223, 2023.\n",
      "[4] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\n",
      "P. Kambadur, D. Rosenberg, and G. Mann, “BloombergGPT: A Large\n",
      "Language Model for Finance,” arXiv preprint arXiv:2303.17564, 2023.\n",
      "[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
      "Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\n",
      "Neural Information Processing Systems, vol. 30, pp. 5998–6008, 2017.\n",
      "[6] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\n",
      "and W. Chen, “Lora: Low-rank adaptation of large language models,”\n",
      "arXiv preprint arXiv:2106.09685, 2021.\n",
      "[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\n",
      "C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\n",
      "models to follow instructions with human feedback,” Advances in Neural\n",
      "Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\n",
      "[8] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language\n",
      "models in retrieval-augmented generation,” in Proceedings of the AAAI\n",
      "Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754–\n",
      "17 762.\n",
      "[9] Z. Cao, W. Li, S. Li, and F. Wei, “Retrieve, rerank and rewrite:\n",
      "Soft template based neural summarization,” in Proceedings of the 56th\n",
      "Annual Meeting of the Association for Computational Linguistics, 2018,\n",
      "pp. 152–161.\n",
      "[10] A. Balaguer, V. Benara, R. L. de Freitas Cunha, R. d. M. Estev˜\n",
      "ao Filho,\n",
      "T. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O.\n",
      "Nunes et al., “Rag vs Fine-tuning: Pipelines, Tradeoffs, and a Case\n",
      "Study on Agriculture,” arXiv e-prints, pp. arXiv–2401, 2024.\n",
      "[11] OpenAI.\n",
      "(2023).\n",
      "[Online].\n",
      "Available:\n",
      "https://openai.com/index/\n",
      "gpt-4-research/\n",
      "[12] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction tuning with\n",
      "GPT-4,” arXiv preprint arXiv:2304.03277, 2023.\n",
      "[13] D. Patel and D. Nishball. (2024) Groq inference tokenomics: Speed,\n",
      "but at what cost? [Online]. Available: https://www.semianalysis.com/p/\n",
      "groq-inference-tokenomics-speed-but\n",
      "[14] M.\n",
      "Agarwal,\n",
      "A.\n",
      "Qureshi,\n",
      "N.\n",
      "Sardana,\n",
      "L.\n",
      "Li,\n",
      "J.\n",
      "Quevedo,\n",
      "and D. Khudia. (2024) LLM inference performance engineering:\n",
      "Best practices. [Online]. Available: https://www.databricks.com/blog/\n",
      "llm-inference-performance-engineering-best-practices\n",
      "[15] H. Dong and Y. Liu, “Metaverse meets consumer electronics,” IEEE\n",
      "Consumer Electronics Magazine, vol. 12, no. 3, pp. 17–19, 2023.\n",
      "[16] Z. Long, H. Dong, and A. El Saddik, “Human-centric resource allocation\n",
      "for the metaverse with multiaccess edge computing,” IEEE Internet of\n",
      "Things Journal, vol. 10, no. 22, pp. 19 993–20 005, 2023.\n",
      "[17] H. Dong, Y. Liu, T. Chu, and A. El Saddik, “Bringing robots home: The\n",
      "rise of AI robots in consumer electronics,” IEEE Consumer Electronics\n",
      "Magazine, vol. 13, 2024.\n",
      "[18] “The future of AI is hybrid,” Qualcomm, Tech. Rep., 2023.\n",
      "[19] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, D. Brooks,\n",
      "and C.-J. Wu, “Chasing carbon: The elusive environmental footprint of\n",
      "computing,” in Proceedings of the IEEE International Symposium on\n",
      "High-Performance Computer Architecture.\n",
      "IEEE, 2021, pp. 854–867.\n",
      "[20] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quanti-\n",
      "fying the carbon emissions of machine learning,” arXiv preprint\n",
      "arXiv:1910.09700, 2019.\n",
      "[21] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and\n",
      "L. Jiang, “LLMCarbon: Modeling the end-to-end carbon footprint of\n",
      "large language models,” arXiv preprint arXiv:2309.14393, 2023.\n",
      "\n",
      "Question: What metrics are used to evaluate the quality of experience (QoE) for users of large language model (LLM) services?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = query\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "prompt = template.format(context = context, question = question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rag chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(api_key=openai_api_key)\n",
    "custom_rag_template = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create contextualised prompt\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contexttualised_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualised_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contexttualised_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create history aware retriever\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualised_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create system prompt\n",
    "\n",
    "system_prompt_template = \"\"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create qa chain and rag chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, system_template)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage chat_history\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    \n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= \"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "\n",
    "# def coversational_chain(query):\n",
    "#     answer = conversational_rag_chain.invoke(\n",
    "#         {\"input\": query},\n",
    "#         config={\n",
    "#             \"configurable\" : {\"session_id\", \"my_session\"}\n",
    "#         }\n",
    "#     )\n",
    "#     pprint.pprint(answer)\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'QoE stands for Quality of Experience, which refers to the overall '\n",
      "           'satisfaction and usability of a service or product as perceived by '\n",
      "           'end users. In the context of large language models (LLMs), QoE '\n",
      "           'encompasses factors like inference accuracy, latency, device '\n",
      "           'capacity, privacy, and security. Maintaining a high QoE is '\n",
      "           'essential for ensuring that users have a positive and seamless '\n",
      "           'interaction with LLM services.',\n",
      " 'chat_history': [],\n",
      " 'context': [Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='cost-effective yet usable CPUs for the inference business.\\nIV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\\nCOMPROMISE OF PERFORMANCE AND COST\\nTokenomics is a compound word for token and economics\\nreferring to the analysis of generative tokens in LLM infer-\\nence from the perspective of economics. Here, we usually\\nconsider two aspects: throughput (tokens per second) and\\nprice (USD per 1 million tokens). According to the up-to-\\ndate data [13], the throughput of most unicorn companies\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n4\\nFig. 4.\\nThe hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\\nbalancing factors, including inference accuracy, latency, device capacity, privacy, and security.\\n(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\\nFireworks, Groq, Leption) lies about 50 to 200 tokens per\\nsecond whereas Groq leads the benchmark to be more than\\n400 tokens per second. In regard to prices, most of the', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='100 milliseconds per token, equivalent to 10 tokens per\\nsecond, leading to 600 words per minute that is faster\\nthan most of the users can read.\\n• Latency:\\nThe\\noverall\\ntime\\nof\\nreceiving\\nan\\nan-\\nswer\\ncomposed\\nof\\ntokens\\nafter\\nthe\\ninitial\\nquery\\nfrom\\nthe\\nusers.\\nTo\\nbe\\nmore\\nprecise,\\nit\\ncan\\nbe\\ndescribed\\nas\\nLatency\\n=\\nTTFT\\n+ TOPT\\n∗\\n(number of total generated tokens).\\n• Throughput: The generated tokens per second. It can\\nbe considered as a median value in a time window\\nconsidering applicable queries and users.\\nIt is important for users to receive high QoE while using\\nthe LLM service. However, better service requires more com-\\nputational and networking resources. It is speculated that the\\nLLM service will be provided with different levels relating\\nto various subscription options or an one-time charge from\\nusers. It would be finally a compromise decision of users in\\nthe consideration of performance and cost.\\nV. HYBRID LLM: GRAVITATING TOWARDS THE EDGE', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
      " 'input': 'What is QoE?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is QoE?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='cost-effective yet usable CPUs for the inference business.\\nIV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\\nCOMPROMISE OF PERFORMANCE AND COST\\nTokenomics is a compound word for token and economics\\nreferring to the analysis of generative tokens in LLM infer-\\nence from the perspective of economics. Here, we usually\\nconsider two aspects: throughput (tokens per second) and\\nprice (USD per 1 million tokens). According to the up-to-\\ndate data [13], the throughput of most unicorn companies\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n4\\nFig. 4.\\nThe hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\\nbalancing factors, including inference accuracy, latency, device capacity, privacy, and security.\\n(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\\nFireworks, Groq, Leption) lies about 50 to 200 tokens per\\nsecond whereas Groq leads the benchmark to be more than\\n400 tokens per second. In regard to prices, most of the', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='100 milliseconds per token, equivalent to 10 tokens per\\nsecond, leading to 600 words per minute that is faster\\nthan most of the users can read.\\n• Latency:\\nThe\\noverall\\ntime\\nof\\nreceiving\\nan\\nan-\\nswer\\ncomposed\\nof\\ntokens\\nafter\\nthe\\ninitial\\nquery\\nfrom\\nthe\\nusers.\\nTo\\nbe\\nmore\\nprecise,\\nit\\ncan\\nbe\\ndescribed\\nas\\nLatency\\n=\\nTTFT\\n+ TOPT\\n∗\\n(number of total generated tokens).\\n• Throughput: The generated tokens per second. It can\\nbe considered as a median value in a time window\\nconsidering applicable queries and users.\\nIt is important for users to receive high QoE while using\\nthe LLM service. However, better service requires more com-\\nputational and networking resources. It is speculated that the\\nLLM service will be provided with different levels relating\\nto various subscription options or an one-time charge from\\nusers. It would be finally a compromise decision of users in\\nthe consideration of performance and cost.\\nV. HYBRID LLM: GRAVITATING TOWARDS THE EDGE', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
       " 'answer': 'QoE stands for Quality of Experience, which refers to the overall satisfaction and usability of a service or product as perceived by end users. In the context of large language models (LLMs), QoE encompasses factors like inference accuracy, latency, device capacity, privacy, and security. Maintaining a high QoE is essential for ensuring that users have a positive and seamless interaction with LLM services.'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_chain(\"What is QoE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'In model training, the concept of Quality of Experience (QoE) can '\n",
      "           'be used as a guiding principle to optimize the performance and '\n",
      "           'efficiency of large language models (LLMs). By considering QoE '\n",
      "           'during training, developers can focus on improving factors that '\n",
      "           'directly impact user satisfaction, such as speed, accuracy, and '\n",
      "           'cost-effectiveness.\\n'\n",
      "           '\\n'\n",
      "           'For instance, when training LLMs, developers may prioritize '\n",
      "           \"enhancing the model's ability to generate accurate and relevant \"\n",
      "           'responses within a reasonable time frame (latency). This focus on '\n",
      "           'QoE can lead to models that provide more useful and timely '\n",
      "           'information to users, ultimately enhancing their overall '\n",
      "           'experience.\\n'\n",
      "           '\\n'\n",
      "           'Moreover, by balancing performance improvements with cost '\n",
      "           'considerations, developers can ensure that the trained models '\n",
      "           'deliver high-quality results while maintaining affordability for '\n",
      "           'both service providers and end users. This approach helps in '\n",
      "           'creating LLMs that not only excel in performance but also offer a '\n",
      "           'satisfactory user experience at a reasonable cost.',\n",
      " 'chat_history': [HumanMessage(content='What is QoE?'),\n",
      "                  AIMessage(content='QoE stands for Quality of Experience, which refers to the overall satisfaction and usability of a service or product as perceived by end users. In the context of large language models (LLMs), QoE encompasses factors like inference accuracy, latency, device capacity, privacy, and security. Maintaining a high QoE is essential for ensuring that users have a positive and seamless interaction with LLM services.')],\n",
      " 'context': [Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in Neural\\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\\n[8] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language\\nmodels in retrieval-augmented generation,” in Proceedings of the AAAI\\nConference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754–\\n17 762.\\n[9] Z. Cao, W. Li, S. Li, and F. Wei, “Retrieve, rerank and rewrite:\\nSoft template based neural summarization,” in Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics, 2018,\\npp. 152–161.\\n[10] A. Balaguer, V. Benara, R. L. de Freitas Cunha, R. d. M. Estev˜\\nao Filho,\\nT. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O.\\nNunes et al., “Rag vs Fine-tuning: Pipelines, Tradeoffs, and a Case\\nStudy on Agriculture,” arXiv e-prints, pp. arXiv–2401, 2024.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
      " 'input': 'how is it used in model training?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how is it used in model training?',\n",
       " 'chat_history': [HumanMessage(content='What is QoE?'),\n",
       "  AIMessage(content='QoE stands for Quality of Experience, which refers to the overall satisfaction and usability of a service or product as perceived by end users. In the context of large language models (LLMs), QoE encompasses factors like inference accuracy, latency, device capacity, privacy, and security. Maintaining a high QoE is essential for ensuring that users have a positive and seamless interaction with LLM services.')],\n",
       " 'context': [Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in Neural\\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\\n[8] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language\\nmodels in retrieval-augmented generation,” in Proceedings of the AAAI\\nConference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754–\\n17 762.\\n[9] Z. Cao, W. Li, S. Li, and F. Wei, “Retrieve, rerank and rewrite:\\nSoft template based neural summarization,” in Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics, 2018,\\npp. 152–161.\\n[10] A. Balaguer, V. Benara, R. L. de Freitas Cunha, R. d. M. Estev˜\\nao Filho,\\nT. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O.\\nNunes et al., “Rag vs Fine-tuning: Pipelines, Tradeoffs, and a Case\\nStudy on Agriculture,” arXiv e-prints, pp. arXiv–2401, 2024.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
       " 'answer': \"In model training, the concept of Quality of Experience (QoE) can be used as a guiding principle to optimize the performance and efficiency of large language models (LLMs). By considering QoE during training, developers can focus on improving factors that directly impact user satisfaction, such as speed, accuracy, and cost-effectiveness.\\n\\nFor instance, when training LLMs, developers may prioritize enhancing the model's ability to generate accurate and relevant responses within a reasonable time frame (latency). This focus on QoE can lead to models that provide more useful and timely information to users, ultimately enhancing their overall experience.\\n\\nMoreover, by balancing performance improvements with cost considerations, developers can ensure that the trained models deliver high-quality results while maintaining affordability for both service providers and end users. This approach helps in creating LLMs that not only excel in performance but also offer a satisfactory user experience at a reasonable cost.\"}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_chain(\"how is it used in model training?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def conversational_chain(query):\n",
    "    answer = conversational_rag_chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"my_session00001\"}\n",
    "        }\n",
    "    )\n",
    "    pprint.pprint(answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Tokenomics refers to the analysis of generative tokens in Large '\n",
      "           'Language Models (LLMs) inference from an economic perspective. It '\n",
      "           'involves considering two key aspects: throughput (tokens per '\n",
      "           'second) and price (USD per 1 million tokens). Throughput measures '\n",
      "           'how many tokens an LLM can generate in a second, while price '\n",
      "           'reflects the cost of generating a certain number of tokens.\\n'\n",
      "           '\\n'\n",
      "           'For example, if a company can produce 100 tokens per second at a '\n",
      "           'cost of $0.5 USD per million tokens, its tokenomics would involve '\n",
      "           'both the efficiency of token generation (throughput) and the '\n",
      "           'cost-effectiveness of producing tokens (price).',\n",
      " 'chat_history': [],\n",
      " 'context': [Document(page_content='cost-effective yet usable CPUs for the inference business.\\nIV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\\nCOMPROMISE OF PERFORMANCE AND COST\\nTokenomics is a compound word for token and economics\\nreferring to the analysis of generative tokens in LLM infer-\\nence from the perspective of economics. Here, we usually\\nconsider two aspects: throughput (tokens per second) and\\nprice (USD per 1 million tokens). According to the up-to-\\ndate data [13], the throughput of most unicorn companies\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n4\\nFig. 4.\\nThe hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\\nbalancing factors, including inference accuracy, latency, device capacity, privacy, and security.\\n(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\\nFireworks, Groq, Leption) lies about 50 to 200 tokens per\\nsecond whereas Groq leads the benchmark to be more than\\n400 tokens per second. In regard to prices, most of the', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='400 tokens per second. In regard to prices, most of the\\naforementioned companies can achieve between $0.2 USD to\\n$1.0 USD per 1 million tokens. Perplexity and Groq lead the\\nboard herein around $0.25 USD.\\nIf we consider the LLM inference as a service, the cus-\\ntomers are users who ask questions by prompts and receive\\nanswers in a paid (e.g., OpenAI GPT-4) or unpaid way (e.g.,\\nOpenAI GPT-3.5 or GPT-4o). Each data center with thousands\\nof or even tens of thousands of AI servers is like a factory\\nwhose products are tokens. During this service, the metrics to\\nevaluate the quality of experience (QoE) of end users include\\n[14]:\\n• Time to first token (TTFT): The waiting time of the\\nusers to receive the response after entering their query.\\nTypically, one second of TTFT is considered to be good\\nenough.\\n• Time per output token (TPOT): The time to receive each\\ngenerated token by the users. If we consider a TPOT as\\n100 milliseconds per token, equivalent to 10 tokens per', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
      "             Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
      " 'input': 'Tell me about tokenomics'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about tokenomics',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='cost-effective yet usable CPUs for the inference business.\\nIV. TOKENOMICS VS. QUALITY OF EXPERIENCE: THE\\nCOMPROMISE OF PERFORMANCE AND COST\\nTokenomics is a compound word for token and economics\\nreferring to the analysis of generative tokens in LLM infer-\\nence from the perspective of economics. Here, we usually\\nconsider two aspects: throughput (tokens per second) and\\nprice (USD per 1 million tokens). According to the up-to-\\ndate data [13], the throughput of most unicorn companies\\nCTSOC NEWS ON CONSUMER TECHNOLOGY\\n4\\nFig. 4.\\nThe hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by\\nbalancing factors, including inference accuracy, latency, device capacity, privacy, and security.\\n(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra,\\nFireworks, Groq, Leption) lies about 50 to 200 tokens per\\nsecond whereas Groq leads the benchmark to be more than\\n400 tokens per second. In regard to prices, most of the', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='400 tokens per second. In regard to prices, most of the\\naforementioned companies can achieve between $0.2 USD to\\n$1.0 USD per 1 million tokens. Perplexity and Groq lead the\\nboard herein around $0.25 USD.\\nIf we consider the LLM inference as a service, the cus-\\ntomers are users who ask questions by prompts and receive\\nanswers in a paid (e.g., OpenAI GPT-4) or unpaid way (e.g.,\\nOpenAI GPT-3.5 or GPT-4o). Each data center with thousands\\nof or even tens of thousands of AI servers is like a factory\\nwhose products are tokens. During this service, the metrics to\\nevaluate the quality of experience (QoE) of end users include\\n[14]:\\n• Time to first token (TTFT): The waiting time of the\\nusers to receive the response after entering their query.\\nTypically, one second of TTFT is considered to be good\\nenough.\\n• Time per output token (TPOT): The time to receive each\\ngenerated token by the users. If we consider a TPOT as\\n100 milliseconds per token, equivalent to 10 tokens per', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       "  Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})],\n",
       " 'answer': 'Tokenomics refers to the analysis of generative tokens in Large Language Models (LLMs) inference from an economic perspective. It involves considering two key aspects: throughput (tokens per second) and price (USD per 1 million tokens). Throughput measures how many tokens an LLM can generate in a second, while price reflects the cost of generating a certain number of tokens.\\n\\nFor example, if a company can produce 100 tokens per second at a cost of $0.5 USD per million tokens, its tokenomics would involve both the efficiency of token generation (throughput) and the cost-effectiveness of producing tokens (price).'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_chain(\"Tell me about tokenomics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conversational_chain(query):\n",
    "    answer = conversational_rag_chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"my_session00001\"}\n",
    "        }\n",
    "    )['answer']\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokenomics in Large Language Models (LLMs) is related to carbon emissions through the operational footprint of LLMs. The operational footprint includes emissions from the energy consumption of hardware during pre-training, fine-tuning, and inference, which are all part of the token generation process in LLMs.\\n\\nWhen optimizing tokenomics in LLMs, developers and researchers aim to strike a balance between performance and cost efficiency. By understanding the relationship between token generation (throughput and price) and energy consumption, they can make more environmentally conscious decisions to reduce the carbon footprint associated with operating LLMs.\\n\\nFor instance, by optimizing token generation efficiency, developers can potentially decrease the energy consumption required for LLM operations, thereby reducing the overall carbon emissions associated with running these models.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_chain(\"How is it related to llm carbon emission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def conversational_rag(query, context=None):\n",
    "    return conversational_rag_chain.invoke(\n",
    "    {\"input\": query},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },)[\"answer\"]\n",
    "\n",
    "rag_demo = gr.ChatInterface(\n",
    "    conversational_rag,\n",
    "    title=\"RAG demo\",\n",
    "    chatbot=gr.Chatbot(height= 300),\n",
    "    textbox= gr.Textbox(placeholder=\"Enter query here\", scale=5),\n",
    "    examples=[\"How does the introduction of GPT-4o by OpenAI, represent a pivotal advancement in the evolution of large language models, and what potential applications could arise from these expanded functionalities?\", \"What are the primary advantages and challenges of deploying LLMs using RAG compared to fine-tuning, particularly in terms of handling specialized knowledge and reducing hallucinations?\"],\n",
    "    clear_btn=gr.Button(\"Clear\"),\n",
    "    undo_btn=gr.Button(\"Undo\"),\n",
    "    retry_btn=gr.Button(\"Retry\"),\n",
    "    submit_btn=gr.Button(\"Submit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
