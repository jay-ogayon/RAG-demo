{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query = \"2405.17147\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chunk data\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunk_data = text_splitter.split_documents(docs)\n",
    "len(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001923BAD6B70>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001923BAD40B0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Pineconde Index\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"pinecone-vector\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec= ServerlessSpec(cloud=\"aws\", region= \"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vector store \n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore.from_documents(chunk_data, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devrpa.operations\\Documents\\RAG\\RAG Demo\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       " Document(page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}),\n",
       " Document(page_content='mlco2 is 69% higher than the actual, because mlco2 assumes\\nall devices consistently operate at the peak computing through-\\nput using the peak power.\\nThe sustainability of LLMs can be viewed through a\\ntwo-fold lens: economic and environmental. LLMs achieve\\neconomic sustainability if the value they generate for orga-\\nnizations (e.g., enhanced efficiency, and improved customer\\nservice) exceeds the costs incurred from training, inference,\\nand hardware maintenance. Environmental sustainability re-\\nquires a multifaceted approach, encompassing renewable en-\\nergy sources for data centers, energy-efficient model architec-\\ntures, and hardware designed for low-power AI workloads. By\\nprioritizing both economic and environmental considerations,\\nLLM development can become a powerful force for positive\\nchange, driving innovation while minimizing its environmental\\nimpact.\\nHaiwei Dong (haiwei.dong@ieee.org) is currently\\na Director and Principal Researcher with Huawei', metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What metrics are used to evaluate the quality of experience (QoE) for users of large language model (LLM) service\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":3})\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the primary advantages of using Retrieval-Augmented Generation (RAG)?\"\n",
    "context = \"RAG combines retrieval mechanisms with generative models to provide accurate and relevant responses by fetching information from external sources and generating coherent answers.\"\n",
    "\n",
    "prompt = template.format(context = context, question = question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=openai_api_key)\n",
    "\n",
    "custom_rag_template = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"With the expanded functionalities of GPT-4o, potential applications could include:\\n\\n1. Enhanced virtual assistants: GPT-4o's multi-modality capabilities could improve the user experience of virtual assistants by enabling them to generate not only text-based responses but also images, videos, audio, and more in a more natural and intuitive manner.\\n\\n2. Content creation: GPT-4o could be used to generate diverse types of content, such as creating image captions, summarizing videos, or even generating audio content based on text descriptions. This could streamline content creation processes for various industries like media, marketing, and entertainment.\\n\\n3. Accessibility tools: The text-to-audio/sound functionality of GPT-4o could be leveraged to create human-like speech or various environmental sounds from text descriptions, making it a valuable tool for enhancing accessibility for individuals with visual impairments or other disabilities.\\n\\n4. Storytelling technologies: GPT-4o's text-to-video functionality could be utilized in creating interactive storytelling experiences, where users can input text descriptions and have them translated into engaging video content, resembling a personalized movie or story.\\n\\nOverall, GPT-4o's expanded functionalities open up a wide range of possibilities for innovative applications across various industries, ultimately enhancing human-computer interaction and content creation processes.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what potential applications could arise from gpt-4o expanded functionalities?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradio\n",
    "import gradio as gr\n",
    "\n",
    "def llm_response(query, context = None):\n",
    "    return rag_chain.invoke(query)\n",
    "\n",
    "rag_demo = gr.ChatInterface(\n",
    "                llm_response,\n",
    "                chatbot=gr.Chatbot(height=300),\n",
    "                textbox=gr.Textbox(placeholder=\"Enter query here\", container=False, scale=5),\n",
    "                title=\"RAG Demo\",\n",
    "                examples=[\"How does the introduction of GPT-4o by OpenAI, represent a pivotal advancement in the evolution of large language models, and what potential applications could arise from these expanded functionalities?\", \"What are the primary advantages and challenges of deploying LLMs using RAG compared to fine-tuning, particularly in terms of handling specialized knowledge and reducing hallucinations?\"],\n",
    "                cache_examples=False,\n",
    "                retry_btn=\"Retry\",\n",
    "                undo_btn=\"Undo\",\n",
    "                clear_btn=\"Clear\",\n",
    "                submit_btn=\"Submit\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
